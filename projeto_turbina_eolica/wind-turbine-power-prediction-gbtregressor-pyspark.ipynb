{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Turbine Power Prediction\n",
    "\n",
    "In this study I am going to predict a wind turbine power production by using the wind speed, wind direction, month and hour data.\n",
    "\n",
    "The dataset consists of 50530 observations. In order to demonstrate my data science skills with big data, I am going to use Pyspark library.\n",
    "\n",
    "The dataset contains:\n",
    "\n",
    "* Date/Time (for 10 minutes intervals)\n",
    "* LV ActivePower (kW): The power generated by the turbine for that moment\n",
    "* Wind Speed (m/s): The wind speed at the hub height of the turbine (the wind speed that turbine use for electricity generation)\n",
    "* TheoreticalPowerCurve (KWh): The theoretical power values that the turbine generates with that wind speed which is given by the turbine manufacturer\n",
    "* Wind Direction (Â°): The wind direction at the hub height of the turbine (wind turbines turn to this direction automaticly)\n",
    "\n",
    "To see my Tableau dashboard:\n",
    "\n",
    "https://public.tableau.com/views/WindTurbineProject/Dashboard1?:language=en&:display_count=y&:origin=viz_share_link\n",
    "\n",
    "Dataset Resource: \n",
    "\n",
    "https://www.kaggle.com/berkerisen/wind-turbine-scada-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of the Study:\n",
    "\n",
    "**My aim is to predict wind turbine power production from the wind speed, wind direction, month of the year and the hour of the day.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "Collecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805913 sha256=258b21541cd818f9022327865364d2b375098aa62abb0822e145970e034d3622\n",
      "  Stored in directory: c:\\users\\kitaw\\appdata\\local\\pip\\cache\\wheels\\23\\f6\\d3\\110e53bd43baeb8d7d38049733d48e39cbecd056f01dba7ee8\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Downloading pyspark library\n",
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x2beee7ff) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x2beee7ff\r\n\tat org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213)\r\n\tat org.apache.spark.storage.StorageUtils$.<clinit>(StorageUtils.scala)\r\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:110)\r\n\tat org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348)\r\n\tat org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287)\r\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:336)\r\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191)\r\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e207c8ae91ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Configuration of Spark Session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wind_turbine_project\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0m\u001b[0;32m    147\u001b[0m                           conf, jsc, profiler_cls)\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \"\"\"\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1573\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1574\u001b[0m             answer, self._gateway_client, None, self._fqn)\n\u001b[0;32m   1575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x2beee7ff) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x2beee7ff\r\n\tat org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213)\r\n\tat org.apache.spark.storage.StorageUtils$.<clinit>(StorageUtils.scala)\r\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:110)\r\n\tat org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348)\r\n\tat org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287)\r\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:336)\r\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191)\r\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n"
     ]
    }
   ],
   "source": [
    "# Importing pyspark libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Configuration of Spark Session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"wind_turbine_project\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset as Spark DataFrame\n",
    "spark_df = spark.read.csv('../input/wind-turbine-scada-dataset/T1.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Caching the dataset\n",
    "spark_df.cache()\n",
    "\n",
    "# Converting all the column names to lower case\n",
    "spark_df = spark_df.toDF(*[c.lower() for c in spark_df.columns])\n",
    "\n",
    "print('Show the first 5 rows')\n",
    "print(spark_df.show(5))\n",
    "print()\n",
    "print('What are the variable data types?')\n",
    "print(spark_df.printSchema())\n",
    "print()\n",
    "print('How many observations do we have?')\n",
    "print(spark_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting a substring from columns to create month and hour variables\n",
    "\n",
    "from pyspark.sql.functions import substring\n",
    "spark_df = spark_df.withColumn(\"month\", substring(\"date/time\", 4,2))\n",
    "spark_df = spark_df.withColumn(\"hour\", substring(\"date/time\", 12,2))\n",
    "\n",
    "# Converting string month and hour variables to integer\n",
    "from pyspark.sql.types import IntegerType\n",
    "spark_df = spark_df.withColumn('month', spark_df.month.cast(IntegerType()))\n",
    "spark_df = spark_df.withColumn('hour', spark_df.hour.cast(IntegerType()))\n",
    "\n",
    "print(spark_df.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "spark_df.select('wind speed (m/s)', 'theoretical_power_curve (kwh)', 'lv activepower (kw)').toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are the distributions of the variables?\n",
    "\n",
    "**For creating visualization we need to either use aggregated data or use a sample from the big data.**\n",
    "\n",
    "**So I will get a random sample from my big data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a random sample from the big data\n",
    "sample_df = spark_df.sample(withReplacement=False, fraction=0.1, seed=42).toPandas()\n",
    "\n",
    "# Visualizing the distributions with the sample data\n",
    "columns = ['wind speed (m/s)', 'wind direction (Â°)', 'month', 'hour', 'theoretical_power_curve (kwh)', 'lv activepower (kw)']\n",
    "i=1\n",
    "plt.figure(figsize=(10,12))\n",
    "for each in columns:\n",
    "    plt.subplot(3,2,i)\n",
    "    sample_df[each].plot.hist(bins=12)\n",
    "    plt.title(each)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Is there any difference between the months for average power production ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average power production by month\n",
    "monthly = spark_df.groupby('month').mean('lv activepower (kw)').sort('avg(lv activepower (kw))').toPandas()\n",
    "sns.barplot(x='month', y='avg(lv activepower (kw))', data=monthly)\n",
    "plt.title('Months and Average Power Production');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Is there any difference between the hours for average power production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average power production by hour\n",
    "hourly = spark_df.groupby('hour').mean('lv activepower (kw)').sort('avg(lv activepower (kw))').toPandas()\n",
    "sns.barplot(x='hour', y='avg(lv activepower (kw))', data=hourly)\n",
    "plt.title('Hours and Average Power Production');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Is there any correlation between the wind speed, wind direction and power production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_df[columns].corr())\n",
    "sns.pairplot(sample_df[columns], markers='*');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wind speed and power production is highly correlated as one would expect.**\n",
    "\n",
    "**We can see there are lower level power production for some wind directions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is the average power production level for different wind speeds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding average power production for 5 m/s wind speed increments\n",
    "wind_speed = []\n",
    "avg_power = []\n",
    "for i in [0,5,10,15,20]:\n",
    "    avg_value = spark_df.filter((spark_df['wind speed (m/s)'] > i) \n",
    "                                & (spark_df['wind speed (m/s)'] <= i+5))\\\n",
    "                                .agg({'lv activepower (kw)':'mean'}).collect()[0][0] \n",
    "    avg_power.append(avg_value)\n",
    "    wind_speed.append(str(i) + '-' + str(i+5))\n",
    "\n",
    "sns.barplot(x=wind_speed, y=avg_power, color='orange')\n",
    "plt.title('Avg Power Production for 5 m/s Wind Speed Increments')\n",
    "plt.xlabel('Wind Speed')\n",
    "plt.ylabel('Average Power Production');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the graph above we can see the power production reaches near a maximum level after the wind speed reaches 15 m/s.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is the power production for different wind directions and speeds? \n",
    "\n",
    "**Let's create a polar diagram with wind speed, wind direction and power production from the sample data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the polar diagram\n",
    "from math import radians\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "# Inside circles are the wind speed and marker color and size represents the amount of power production\n",
    "sns.scatterplot(x=[radians(x) for x in sample_df['wind direction (Â°)']], \n",
    "                y=sample_df['wind speed (m/s)'],\n",
    "                size=sample_df['lv activepower (kw)'],\n",
    "                hue=sample_df['lv activepower (kw)'],\n",
    "                alpha=0.7, legend=None)\n",
    "# Setting the polar diagram's top represents the North \n",
    "ax.set_theta_zero_location('N')\n",
    "# Setting -1 to start the wind direction clockwise\n",
    "ax.set_theta_direction(-1)\n",
    "# Setting wind speed labels in a better position to see\n",
    "ax.set_rlabel_position(110)\n",
    "plt.title('Wind Speed - Wind Direction - Power Production Diagram')\n",
    "plt.ylabel(None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the wind turbine produces more power if the wind blows from the directions between 000-090 and 180-225 degrees.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Does the manufacturer's theoritical power production curve fit well with the real production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='wind speed (m/s)', y='lv activepower (kw)', color='orange', label='Real Production', alpha=0.5, data=sample_df)\n",
    "sns.lineplot(x='wind speed (m/s)', y='theoretical_power_curve (kwh)', color='blue', label='Theoritical Production', data=sample_df)\n",
    "plt.title('Wind Speed and Power Production Chart')\n",
    "plt.ylabel('Power Production (kw)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the graph above, we can see the theoritical power production curve generally fits well with the real production.**\n",
    "\n",
    "**We can see the power production reaches a maximum level and continues in a straight line if the wind speed reaches to 15 m/s.**\n",
    "\n",
    "**Also we can see there are some 0 power production, even the wind speed is higher than 5 m/s. I want to investigate the reason.**\n",
    "\n",
    "**But before what is the minimum wind speed for theoritical power production curve?**\n",
    "\n",
    "### Question: What is the wind speed threshold value for zero theorical power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the big data where the real and theoritical power productions are equal to 0\n",
    "zero_theo_power = spark_df.filter((spark_df['lv activepower (kw)'] == 0)\n",
    "                                  & (spark_df['theoretical_power_curve (kwh)'] == 0)).toPandas()\n",
    "\n",
    "display(zero_theo_power[['wind speed (m/s)', 'theoretical_power_curve (kwh)', 'lv activepower (kw)']].sample(5))\n",
    "\n",
    "# Let's see the wind speed distribution for 0 power production\n",
    "zero_theo_power['wind speed (m/s)'].hist()\n",
    "plt.title('Wind Speed Distribution for 0 Power Production')\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.ylabel('Counts for 0 Power Production');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see from above, limit for the theoritical power curve is 3 m/s wind speed. If the wind speed is below 3 m/s model doesn't expect any power production.**\n",
    "\n",
    "**But there are some observations for 0 power production even the wind speed is more than 3 m/s.**\n",
    "\n",
    "### Question: Why there aren't any power production in some observations while the wind speed is higher than 3 m/s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations for the wind speed > 3m/s and power production = 0, \n",
    "# While theoritically there should be power production\n",
    "zero_power = spark_df.filter((spark_df['lv activepower (kw)'] == 0)\n",
    "                            & (spark_df['theoretical_power_curve (kwh)'] != 0)\n",
    "                            & (spark_df['wind speed (m/s)'] > 3)).toPandas()\n",
    "display(zero_power.head())\n",
    "print('No of Observations (while Wind Speed > 3 m/s and Power Production = 0): ', len(zero_power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 3497 observations where theoritically there should be power production. From the dataset we cannot see the reason, it might be caused by maintenance. But let's see if we can see any information from the wind speed, direction and month?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_power['wind speed (m/s)'].plot.hist(bins=8)\n",
    "plt.xlabel('Wind Speed (m/s)')\n",
    "plt.ylabel('Counts for Zero Production')\n",
    "plt.title('Wind Speed Counts for Zero Power Production')\n",
    "plt.xticks(ticks=np.arange(4,18,2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It looks like theoritically wind speed threshold should be 4 m/s. But there are also other observations with zero power production while the wind speed is higher.**\n",
    "\n",
    "**Let's see the monthly distribution for zero power production.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(zero_power['month']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is usually in December and January when the wind turbine doesn't produce production.**\n",
    "\n",
    "**Because I cannot decide if these zero power productions are caused by maintenance periods or something else, I am going to accept those 3497 observations as outliers and remove them from the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding the observations meeting the filter criterias \n",
    "spark_df = spark_df.filter(~((spark_df['lv activepower (kw)'] == 0)\n",
    "                            & (spark_df['theoretical_power_curve (kwh)'] != 0)\n",
    "                            & (spark_df['wind speed (m/s)'] > 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Is there any other outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['wind speed (m/s)', 'wind direction (Â°)', 'theoretical_power_curve (kwh)', 'lv activepower (kw)']\n",
    "i=1\n",
    "plt.figure(figsize=(20,3))\n",
    "for each in columns:\n",
    "    df = spark_df.select(each).toPandas()\n",
    "    plt.subplot(1,4,i)\n",
    "    sns.boxplot(df)\n",
    "    plt.title(each)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the graphs above I can see there are some outliers in the wind speed data.**\n",
    "\n",
    "**I will find the upper and lower threshold values for the wind speed data, and I will analyze the outliers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas df for visualization\n",
    "wind_speed = spark_df.select('wind speed (m/s)').toPandas()\n",
    "\n",
    "# Defining the quantiles and interquantile range\n",
    "Q1 = wind_speed['wind speed (m/s)'].quantile(0.25)\n",
    "Q3 = wind_speed['wind speed (m/s)'].quantile(0.75)\n",
    "IQR = Q3-Q1\n",
    "# Defining the lower and upper threshold values\n",
    "lower = Q1 - 1.5*IQR\n",
    "upper = Q3 + 1.5*IQR\n",
    "\n",
    "print('Quantile (0.25): ', Q1, '  Quantile (0.75): ', Q3)\n",
    "print('Lower threshold: ', lower, ' Upper threshold: ', upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy indexing for outliers\n",
    "outlier_tf = (wind_speed['wind speed (m/s)'] < lower) | (wind_speed['wind speed (m/s)'] > upper)\n",
    "\n",
    "print('Total Number of Outliers: ', len(wind_speed['wind speed (m/s)'][outlier_tf]))\n",
    "print('--'*15)\n",
    "print('Some Examples of Outliers:')\n",
    "print(wind_speed['wind speed (m/s)'][outlier_tf].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is a rare event for wind speed to be over 19 m/s in our dataset.**\n",
    "\n",
    "**Out of 47033, there is only 407 observations while the wind speed is over 19 m/s.**\n",
    "\n",
    "**Now I want to see average power production for these high wind speed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_df.select('wind speed (m/s)', 'lv activepower (kw)')\\\n",
    ".filter(spark_df['wind speed (m/s)'] >= 19)\\\n",
    ".agg({'lv activepower (kw)':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So instead of erasing the outliers, I am going to set the wind speed as 19 m/s for those observations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "spark_df = spark_df.withColumn('wind speed (m/s)', \n",
    "                               F.when(F.col('wind speed (m/s)') > 19.447, 19)\n",
    "                               .otherwise(F.col('wind speed (m/s)')))\n",
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are the general criterias for power production?\n",
    "\n",
    "**It is important to understand the pattern in the data. We should learn the data before the machine.**\n",
    "\n",
    "**1. We saw from the graph that in March, August and November, the average power production is higher.**\n",
    "\n",
    "**2. The average power production is higher daily between 16:00 and 24:00.**\n",
    "\n",
    "**3. The power production is higher when the wind blows from the directions between 000-090 and 180-225 degrees.**\n",
    "\n",
    "**So let's try to predict a high and low level of power production from the criterias above before ML algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High level power production\n",
    "spark_df.filter(((spark_df['month'] == 3) | (spark_df['month'] == 8) | (spark_df['month'] == 11)) \n",
    "                & ((spark_df['hour'] >= 16) | (spark_df['hour'] <= 24)) \n",
    "                & ((spark_df['wind direction (Â°)'] > 0) | (spark_df['wind direction (Â°)'] < 90))\n",
    "                & ((spark_df['wind direction (Â°)'] > 180) | (spark_df['wind direction (Â°)'] < 225))\n",
    "               ).agg({'lv activepower (kw)':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low level power production\n",
    "spark_df.filter((spark_df['month'] == 7) \n",
    "                & ((spark_df['hour'] >= 9) | (spark_df['hour'] <= 11)) \n",
    "                & ((spark_df['wind direction (Â°)'] > 90) | (spark_df['wind direction (Â°)'] < 160))\n",
    "               ).agg({'lv activepower (kw)':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for ML Algorithms\n",
    "\n",
    "**After analysing and understanding the dataset, we can build a ML regression model to predict wind turbine power production by using the wind speed, wind direction, month of the year and hour of the day.**\n",
    "\n",
    "**Using ML algorithms with Spark is a bit different from well known Sckitlearn library.**\n",
    "\n",
    "**We need to feed the model with a dataframe made of variables compressed in vectors called as 'features', and target variable as 'label'. For these convertions I am going to use VectorAssembler method from Pyspark.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the independent variables (Features)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Converting lv activepower (kw) variable as label\n",
    "spark_df = spark_df.withColumn('label', spark_df['lv activepower (kw)'])\n",
    "\n",
    "# Defining the variables to be used\n",
    "variables = ['month', 'hour', 'wind speed (m/s)', 'wind direction (Â°)']\n",
    "vectorAssembler = VectorAssembler(inputCols = variables, outputCol = 'features')\n",
    "va_df = vectorAssembler.transform(spark_df)\n",
    "\n",
    "# Combining features and label column\n",
    "final_df = va_df.select('features', 'label')\n",
    "final_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "**Now we can split our dataset into train and test datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = final_df.randomSplit([0.8, 0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "\n",
    "print('Train dataset: ', train_df.count())\n",
    "print('Test dataset : ', test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Initial Model\n",
    "\n",
    "**I am going to use GBT regressor for this study.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Creating the gbm regressor object\n",
    "gbm = GBTRegressor(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Training the model with train data\n",
    "gbm_model = gbm.fit(train_df)\n",
    "\n",
    "# Predicting using the test data\n",
    "y_pred = gbm_model.transform(test_df)\n",
    "\n",
    "# Initial look at the target and predicted values\n",
    "y_pred.select('label', 'prediction').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's evaluate our model's success.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model success\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='label')\n",
    "\n",
    "print('R2 SCORE : ', evaluator.evaluate(y_pred, {evaluator.metricName: 'r2'}))\n",
    "print('MAE      : ', evaluator.evaluate(y_pred, {evaluator.metricName: 'mae'}))\n",
    "print('RMSE     : ', evaluator.evaluate(y_pred, {evaluator.metricName: 'rmse'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R2 score means, real power production's 97% variability can be explained by the ML model.**\n",
    "\n",
    "**MAE is the mean absolute difference between the real and predicted power production.**\n",
    "\n",
    "**RMSE is the square root of mean squared difference between the real and predicted values.**\n",
    "\n",
    "**Even though the R2 is high, we should also check the MAE and RMSE values with the real value's summary statistics.**\n",
    "\n",
    "**One can tune the hyperparameters to increase the model success. But I this look good enough for me.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Real, Theoritical and Predicted Power Productions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I am going to use sample_df for comparing the actual, theoritical and the model power productions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting sample_df back to Spark dataframe\n",
    "eva_df = spark.createDataFrame(sample_df)\n",
    "\n",
    "# Converting lv activepower (kw) variable as label\n",
    "eva_df = eva_df.withColumn('label', eva_df['lv activepower (kw)'])\n",
    "\n",
    "# Defining the variables to be used\n",
    "variables = ['month', 'hour', 'wind speed (m/s)', 'wind direction (Â°)']\n",
    "vectorAssembler = VectorAssembler(inputCols = variables, outputCol = 'features')\n",
    "vec_df = vectorAssembler.transform(eva_df)\n",
    "\n",
    "# Combining features and label column\n",
    "vec_df = vec_df.select('features', 'label')\n",
    "\n",
    "# Using ML model to predict\n",
    "preds = gbm_model.transform(vec_df)\n",
    "preds_df = preds.select('label','prediction').toPandas()\n",
    "\n",
    "# Compining dataframes to compare\n",
    "frames = [sample_df[['wind speed (m/s)', 'theoretical_power_curve (kwh)']], preds_df]\n",
    "sample_data = pd.concat(frames, axis=1)\n",
    "\n",
    "# Visualizing real, theoritical and predicted power production\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.scatterplot(x='wind speed (m/s)', y='label',alpha=0.5, label= 'Real Power', data=sample_data)\n",
    "sns.scatterplot(x='wind speed (m/s)', y='prediction', alpha=0.7, label='Predicted Power', marker='o', data=sample_data)\n",
    "sns.lineplot(x='wind speed (m/s)', y='theoretical_power_curve (kwh)', label='Theoritical Power',color='purple', data=sample_data)\n",
    "plt.title('Wind Turbine Power Production Prediction')\n",
    "plt.ylabel('Power Production (kw)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the graph above, the model fits better to the real power productions, than the theoritical power production curve.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
