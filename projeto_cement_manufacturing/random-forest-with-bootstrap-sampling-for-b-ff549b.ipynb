{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<h2 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Cement Manufacturing Project</h2> \n<br>","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:30px; color: #007580 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Here are my other notebooks, please have a look and definitely you will find it useful. Happy reading :)</strong></p>\n<ol>\n<li><a href =\"https://www.kaggle.com/vinayakshanawad/industrial-safety-complete-solution\">Industrial Safety - Complete Solution</a></li>\n<li><a href =\"https://www.kaggle.com/vinayakshanawad/eda-statistical-analysis-hypothesis-testing\">EDA - Statistical Analysis - Hypothesis Testing</a></li>\n</ol>","metadata":{}},{"cell_type":"markdown","source":"<a id = '0'></a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #007580; color : #fed049; border-radius: 5px 5px; text-align:center; font-weight: bold\" >Table of Contents</h2> \n\n1. [Overview](#1.0)\n2. [Import the necessary libraries](#2.0)\n3. [Data Collection](#3.0)\n4. [Data Cleaning](#4.0)\n\t- [4.1 Check Duplicates](#4.1)\n\t- [4.2 Drop Duplicates](#4.2)\n\t- [4.3 Check Outliers](#4.3)\n\t- [4.4 Working with Outliers: Correcting, Removing](#4.4)\n\t- [4.5 Check Outliers after correction](#4.5)\n\t- [4.6 Check Missing Values](#4.6)\n5. [EDA (Data Analysis and Preparation)](#5.0)\n\t- [5.1 Variable Identification](#5.1)\n\t- [5.2 Univariate Analysis](#5.2)\n\t- [5.3 Study Summary Statistics](#5.3)\n\t- [5.4 Multivariate Analysis](#5.4)\n\t- [5.5 Study Correlation](#5.5)\n\t- [5.6 EDA (Exploratory Data Analysis) Summary](#5.6)\n6. [Feature Engineering](#6.0)\n\t- [6.1 Variable Creation](#6.1)\n7. [Model Building and Validation](#7.0)\n\t- [7.1 Sampling Techniques - Create Training and Test Set](#7.1)\n\t- [7.2 Decide on complexity of the model, should it be simple linear model in terms of parameters or would a quadratic or higher degree help](#7.2)\n\t- [7.3 Explore for gaussians. If data is likely to be a mix of gaussians, explore individual clusters and presenting my findings in terms of the independent attributes and their suitability to predict strength](#7.3)\n\t- [7.4 Overall Summary - Before feature selection](#7.4)\n8. [Feature Selection Methods](#8.0)\n\t- [8.1 Feature Importance](#8.1)\n\t- [8.2 Overall summary - after feature selection](#8.2)\n\t- [8.3 Comparison of with and without feature selection methods](#8.3)\n9. [Optimization](#9.0)\n\t- [9.1 Hyper Parameter Tuning](#9.1)\n\t- [9.2 Bootstrap Sampling - Model performance range at 95% confidence level](#9.2)\n10. [Conclusion](#10.0)","metadata":{}},{"cell_type":"markdown","source":"<a id = '1.0'></a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 1. Overview </h2> ","metadata":{}},{"cell_type":"markdown","source":"### Data Description:\n\nThe actual concrete compressive strength (MPa) for a given mixture under a\nspecific age (days) was determined from laboratory. Data is in raw form (not scaled). The data has 8 quantitative input variables, and 1 quantitative output variable, and 1030 instances (observations).","metadata":{"id":"l2KjovDsNPmz"}},{"cell_type":"markdown","source":"### Domain:\n\nCement manufacturing","metadata":{"id":"W9HcVXASNPm0"}},{"cell_type":"markdown","source":"### Context\n\nConcrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.","metadata":{"id":"hGD3IjZ0NPm2"}},{"cell_type":"markdown","source":"### Attribute Information:\n    \n* **Cement** : measured in kg in a m3 mixture\n* **Blast** : measured in kg in a m3 mixture\n* **Fly ash** : measured in kg in a m3 mixture\n* **Water** : measured in kg in a m3 mixture\n* **Superplasticizer** : measured in kg in a m3 mixture\n* **Coarse Aggregate** : measured in kg in a m3 mixture\n* **Fine Aggregate** : measured in kg in a m3 mixture\n* **Age** : day (1~365)\n* **Concrete compressive strength** measured in MPa","metadata":{"id":"9POl-wgNNPm4"}},{"cell_type":"markdown","source":"### Objective\n\n**Modeling of strength of high performance concrete using Machine Learning.**","metadata":{"id":"NTlZFyrnNPm6"}},{"cell_type":"markdown","source":"<a id = '2.0'></a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 2. Import the necessary libraries </h2> ","metadata":{}},{"cell_type":"code","source":"#pip install catboost --no-cache-dir","metadata":{"id":"cAe3W633BYGq","outputId":"535aaa9c-59b9-48bc-adff-3ceada335632","execution":{"iopub.status.busy":"2021-07-16T15:29:33.919014Z","iopub.execute_input":"2021-07-16T15:29:33.91966Z","iopub.status.idle":"2021-07-16T15:29:33.924562Z","shell.execute_reply.started":"2021-07-16T15:29:33.91956Z","shell.execute_reply":"2021-07-16T15:29:33.923625Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport itertools\n\nimport time\n\n# used to supress display of warnings\nimport warnings\n\n# ols library\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport missingno as mno\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import OPTICS\n\n# import zscore for scaling the data\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, RobustScaler\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\n\n# pre-processing methods\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.compose import TransformedTargetRegressor\n\n# the regression models \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\n\n# cross-validation methods\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn import metrics\n\nfrom sklearn.pipeline import Pipeline\n\n# feature-selection methods\nfrom sklearn.feature_selection import SelectFromModel\n\n# bootstrap sampling\nfrom sklearn.utils import resample","metadata":{"id":"5siqd3b_NPm7","outputId":"83ebe3a0-f94a-4602-faf7-ecda1f820f3d","execution":{"iopub.status.busy":"2021-07-16T15:29:33.92957Z","iopub.execute_input":"2021-07-16T15:29:33.930134Z","iopub.status.idle":"2021-07-16T15:29:38.139619Z","shell.execute_reply.started":"2021-07-16T15:29:33.930098Z","shell.execute_reply":"2021-07-16T15:29:38.138213Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Setting Options","metadata":{"id":"Oz3KBqEJNPm7"}},{"cell_type":"code","source":"# suppress display of warnings\nwarnings.filterwarnings('ignore')\n\n# display all dataframe columns\npd.options.display.max_columns = None\n\n# to set the limit to 3 decimals\npd.options.display.float_format = '{:.7f}'.format\n\n# display all dataframe rows\npd.options.display.max_rows = None","metadata":{"id":"tOgPQNi1NPnL","execution":{"iopub.status.busy":"2021-07-16T15:29:38.1419Z","iopub.execute_input":"2021-07-16T15:29:38.142338Z","iopub.status.idle":"2021-07-16T15:29:38.148852Z","shell.execute_reply.started":"2021-07-16T15:29:38.14229Z","shell.execute_reply":"2021-07-16T15:29:38.147605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '3.0'></a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 3. Data Collection </h2> ","metadata":{}},{"cell_type":"code","source":"# Reading Concrete data\nconcrete_df = pd.read_csv(\"../input/cement-manufacturing-concrete-dataset/concrete.csv\")","metadata":{"id":"3UZVj2g3NPnL","outputId":"e9d69cef-ec9b-449d-e603-e33515a29183","execution":{"iopub.status.busy":"2021-07-16T15:29:38.151407Z","iopub.execute_input":"2021-07-16T15:29:38.151865Z","iopub.status.idle":"2021-07-16T15:29:38.1877Z","shell.execute_reply.started":"2021-07-16T15:29:38.151798Z","shell.execute_reply":"2021-07-16T15:29:38.18657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the top 5 rows\nconcrete_df.head()","metadata":{"id":"2438DYUJNPna","outputId":"2b3eca00-0c1a-444c-b395-caac0e9fe419","execution":{"iopub.status.busy":"2021-07-16T15:29:38.189676Z","iopub.execute_input":"2021-07-16T15:29:38.190017Z","iopub.status.idle":"2021-07-16T15:29:38.218329Z","shell.execute_reply.started":"2021-07-16T15:29:38.189982Z","shell.execute_reply":"2021-07-16T15:29:38.217191Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Shape of the data </strong></p> ","metadata":{}},{"cell_type":"code","source":"# Get the shape of Concrete data\nconcrete_df.shape","metadata":{"id":"_bxJmjuKNPnq","outputId":"18746623-9b51-4958-e631-5ae1adba935b","execution":{"iopub.status.busy":"2021-07-16T15:29:38.219894Z","iopub.execute_input":"2021-07-16T15:29:38.220197Z","iopub.status.idle":"2021-07-16T15:29:38.228776Z","shell.execute_reply.started":"2021-07-16T15:29:38.220169Z","shell.execute_reply":"2021-07-16T15:29:38.227312Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of rows = {0} and Number of Columns = {1} in Data frame\".format(concrete_df.shape[0],concrete_df.shape[1]))","metadata":{"id":"1_rNzS2XNPnq","outputId":"cf48df57-40e3-4c5f-9252-433cdd7036fa","execution":{"iopub.status.busy":"2021-07-16T15:29:38.231231Z","iopub.execute_input":"2021-07-16T15:29:38.231611Z","iopub.status.idle":"2021-07-16T15:29:38.243093Z","shell.execute_reply.started":"2021-07-16T15:29:38.23158Z","shell.execute_reply":"2021-07-16T15:29:38.241771Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Data type of each attribute </strong></p> ","metadata":{}},{"cell_type":"code","source":"# Check datatypes\nconcrete_df.dtypes","metadata":{"id":"mhXNhrcQNPn9","outputId":"c5c4d502-8d5e-4b1e-924d-dad7f54c56a9","execution":{"iopub.status.busy":"2021-07-16T15:29:38.244567Z","iopub.execute_input":"2021-07-16T15:29:38.245014Z","iopub.status.idle":"2021-07-16T15:29:38.260147Z","shell.execute_reply.started":"2021-07-16T15:29:38.244975Z","shell.execute_reply":"2021-07-16T15:29:38.258743Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the above output, we see that except for the column 'age' all our columns datatype is float64.\n\n#### The data has 8 quantitative input variables and 1 quantitative output variable - Strength","metadata":{"id":"jyNRKQ9ZNPoG"}},{"cell_type":"code","source":"# Check Data frame info\nconcrete_df.info()","metadata":{"id":"WgNnlPt0NPoH","outputId":"94026cde-f288-4caa-bd33-e7c5bf325236","execution":{"iopub.status.busy":"2021-07-16T15:29:38.264179Z","iopub.execute_input":"2021-07-16T15:29:38.26451Z","iopub.status.idle":"2021-07-16T15:29:38.292488Z","shell.execute_reply.started":"2021-07-16T15:29:38.264481Z","shell.execute_reply":"2021-07-16T15:29:38.291042Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column names of Data frame\nconcrete_df.columns","metadata":{"id":"9WYvC8XPNPoO","outputId":"ae52a977-caec-4f3e-9adf-63f3b37684ce","execution":{"iopub.status.busy":"2021-07-16T15:29:38.294967Z","iopub.execute_input":"2021-07-16T15:29:38.295403Z","iopub.status.idle":"2021-07-16T15:29:38.30901Z","shell.execute_reply.started":"2021-07-16T15:29:38.295287Z","shell.execute_reply":"2021-07-16T15:29:38.30726Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '4.0'></a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 4. Data Cleaning </h2> ","metadata":{"id":"EPn_zsj7NPoW"}},{"cell_type":"markdown","source":"<a id = '4.1'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.1 Check Duplicates </strong></p> ","metadata":{}},{"cell_type":"code","source":"# Check duplicates in a data frame\nconcrete_df.duplicated().sum()","metadata":{"id":"5IUo0c6KNPoZ","outputId":"988ec9d5-fe34-42eb-85e8-cb16883e0ac6","execution":{"iopub.status.busy":"2021-07-16T15:29:38.31054Z","iopub.execute_input":"2021-07-16T15:29:38.310973Z","iopub.status.idle":"2021-07-16T15:29:38.332045Z","shell.execute_reply.started":"2021-07-16T15:29:38.310932Z","shell.execute_reply":"2021-07-16T15:29:38.330222Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the duplicate records\nduplicates = concrete_df.duplicated()\n\nconcrete_df[duplicates]","metadata":{"id":"wzhc5_FNNPog","outputId":"e5c70398-5357-45d8-dc59-a730e95fc580","execution":{"iopub.status.busy":"2021-07-16T15:29:38.33345Z","iopub.execute_input":"2021-07-16T15:29:38.333858Z","iopub.status.idle":"2021-07-16T15:29:38.362631Z","shell.execute_reply.started":"2021-07-16T15:29:38.333808Z","shell.execute_reply":"2021-07-16T15:29:38.361488Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There is no need to worry about preserving the data; it is already a part of the concrete dataset and we can merely remove or drop these rows from your cleaned data","metadata":{"id":"wUcmCUIjNPoo"}},{"cell_type":"markdown","source":"<a id = '4.2'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.2 Drop Duplicates </strong></p> ","metadata":{}},{"cell_type":"code","source":"# Delete duplicate rows\nconcrete_df.drop_duplicates(inplace=True)","metadata":{"id":"sIDlqn6SNPor","execution":{"iopub.status.busy":"2021-07-16T15:29:38.363979Z","iopub.execute_input":"2021-07-16T15:29:38.364279Z","iopub.status.idle":"2021-07-16T15:29:38.372688Z","shell.execute_reply.started":"2021-07-16T15:29:38.36425Z","shell.execute_reply":"2021-07-16T15:29:38.371417Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the shape of Concrete data\nconcrete_df.shape","metadata":{"id":"b-attHvdNPo1","outputId":"bf9b8406-89e3-4f71-be1b-f6f081b071c5","execution":{"iopub.status.busy":"2021-07-16T15:29:38.374274Z","iopub.execute_input":"2021-07-16T15:29:38.374646Z","iopub.status.idle":"2021-07-16T15:29:38.384375Z","shell.execute_reply.started":"2021-07-16T15:29:38.374613Z","shell.execute_reply":"2021-07-16T15:29:38.383206Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '4.3'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.3 Check Outliers </strong></p> ","metadata":{}},{"cell_type":"code","source":"# Create a boxplot for all the continuous features\nconcrete_df.boxplot(column = ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age', 'strength'], rot=45, figsize = (20,10));","metadata":{"id":"zYRq0vbJNPo4","outputId":"d01b93e4-37de-4325-ae49-92a9141df5c9","execution":{"iopub.status.busy":"2021-07-16T15:29:38.386239Z","iopub.execute_input":"2021-07-16T15:29:38.386556Z","iopub.status.idle":"2021-07-16T15:29:38.816364Z","shell.execute_reply.started":"2021-07-16T15:29:38.386518Z","shell.execute_reply":"2021-07-16T15:29:38.815604Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Looking at the plot above; Stag, Water, Superplastic, Fineagg, Age and Strength columns have outliers and we need to treat those outliers.**","metadata":{"id":"wNqWNc4UNPo4"}},{"cell_type":"markdown","source":"<a id = '4.4'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.4 Working with Outliers: Correcting, Removing </strong></p> ","metadata":{}},{"cell_type":"code","source":"concrete_df_outliers = pd.DataFrame(concrete_df.loc[:,])\n\n# Calculate IQR\nQ1 = concrete_df_outliers.quantile(0.25)\nQ3 = concrete_df_outliers.quantile(0.75)\nIQR = Q3 - Q1\n\nprint(IQR)","metadata":{"id":"QpcsP93bNPpH","outputId":"fb828b39-f106-4b97-cef2-e96785572cc1","execution":{"iopub.status.busy":"2021-07-16T15:29:38.817731Z","iopub.execute_input":"2021-07-16T15:29:38.818227Z","iopub.status.idle":"2021-07-16T15:29:38.831115Z","shell.execute_reply.started":"2021-07-16T15:29:38.818193Z","shell.execute_reply":"2021-07-16T15:29:38.83018Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** The **first array contains the list of row numbers** and **second array respective column numbers** in concrete_df_outliers data frame","metadata":{"id":"XvZd27XwNPpH"}},{"cell_type":"code","source":"concrete_df.columns","metadata":{"id":"ZwAO1fw8NPpH","outputId":"0e676b3f-ecb5-4293-983b-d45a277ad53b","execution":{"iopub.status.busy":"2021-07-16T15:29:38.83255Z","iopub.execute_input":"2021-07-16T15:29:38.833065Z","iopub.status.idle":"2021-07-16T15:29:38.841665Z","shell.execute_reply.started":"2021-07-16T15:29:38.833031Z","shell.execute_reply":"2021-07-16T15:29:38.840347Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can use IQR score to filter out the outliers by keeping only valid values\n\n# Replace every outlier on the upper side by the upper whisker - for 'water', 'superplastic', \n# 'fineagg', 'age' and 'strength' columns\nfor i, j in zip(np.where(concrete_df_outliers > Q3 + 1.5 * IQR)[0], np.where(concrete_df_outliers > Q3 + 1.5 * IQR)[1]):\n    \n    whisker  = Q3 + 1.5 * IQR\n    concrete_df_outliers.iloc[i,j] = whisker[j]\n    \n# Replace every outlier on the lower side by the lower whisker - for 'water' column\nfor i, j in zip(np.where(concrete_df_outliers < Q1 - 1.5 * IQR)[0], np.where(concrete_df_outliers < Q1 - 1.5 * IQR)[1]): \n    \n    whisker  = Q1 - 1.5 * IQR\n    concrete_df_outliers.iloc[i,j] = whisker[j]","metadata":{"id":"DyKCAsSANPpX","execution":{"iopub.status.busy":"2021-07-16T15:29:38.843115Z","iopub.execute_input":"2021-07-16T15:29:38.843414Z","iopub.status.idle":"2021-07-16T15:29:38.919071Z","shell.execute_reply.started":"2021-07-16T15:29:38.843384Z","shell.execute_reply":"2021-07-16T15:29:38.917865Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove outliers columns - 'water', 'superplastic', 'fineagg', 'age', 'water' and 'strength'\nconcrete_df.drop(columns = concrete_df.loc[:,], inplace = True)","metadata":{"id":"pjmbn7MDNPpX","execution":{"iopub.status.busy":"2021-07-16T15:29:38.92037Z","iopub.execute_input":"2021-07-16T15:29:38.920678Z","iopub.status.idle":"2021-07-16T15:29:38.927233Z","shell.execute_reply.started":"2021-07-16T15:29:38.920639Z","shell.execute_reply":"2021-07-16T15:29:38.926025Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add 'water', 'superplastic', 'fineagg', 'age', 'water' and 'strength' with no outliers from concrete_df_outliers to \n# concrete_df\nconcrete_df = pd.concat([concrete_df, concrete_df_outliers], axis = 1)","metadata":{"id":"Vd0VVSxiNPpX","execution":{"iopub.status.busy":"2021-07-16T15:29:38.928583Z","iopub.execute_input":"2021-07-16T15:29:38.929081Z","iopub.status.idle":"2021-07-16T15:29:38.940949Z","shell.execute_reply.started":"2021-07-16T15:29:38.92905Z","shell.execute_reply":"2021-07-16T15:29:38.940011Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '4.5'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.5 Check Outliers after correction </strong></p> ","metadata":{}},{"cell_type":"code","source":"# Create a boxplot for all the continuous features\nconcrete_df.boxplot(column = ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age', 'strength'], rot=45, figsize = (20,10));","metadata":{"id":"KM6zSublNPpt","outputId":"3f7a4b69-b4b4-4965-e54b-081a1aded7e9","execution":{"iopub.status.busy":"2021-07-16T15:29:38.942111Z","iopub.execute_input":"2021-07-16T15:29:38.942406Z","iopub.status.idle":"2021-07-16T15:29:39.309527Z","shell.execute_reply.started":"2021-07-16T15:29:38.94238Z","shell.execute_reply":"2021-07-16T15:29:39.308246Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Looking at the above plot, there are no more outliers in concrete data set**","metadata":{"id":"BvM2CS9wNPp0"}},{"cell_type":"markdown","source":"<a id = '4.6'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.6 Check Missing Values </strong></p> ","metadata":{}},{"cell_type":"code","source":"# Check the presence of missing values\nconcrete_df.isnull().sum()","metadata":{"id":"coKlXNGONPp3","outputId":"44fbe9e2-affd-4ddb-9738-535804f62315","execution":{"iopub.status.busy":"2021-07-16T15:29:39.311866Z","iopub.execute_input":"2021-07-16T15:29:39.312204Z","iopub.status.idle":"2021-07-16T15:29:39.321845Z","shell.execute_reply.started":"2021-07-16T15:29:39.312173Z","shell.execute_reply":"2021-07-16T15:29:39.320684Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the presence of missing values\nconcrete_df_missval = concrete_df.copy()   # Make a copy of the dataframe\nisduplicates = False\n\nfor x in concrete_df_missval.columns:\n    concrete_df_missval[x] = concrete_df_missval[x].astype(str).str.replace(\".\", \"\")\n    result = concrete_df_missval[x].astype(str).str.isalnum() # Check whether all characters are alphanumeric\n    if False in result.unique():\n        isduplicates = True\n        print('For column \"{}\" unique values are {}'.format(x, concrete_df_missval[x].unique()))\n        print('\\n')\n        \nif not isduplicates:\n    print('No duplicates in this dataset')","metadata":{"id":"ZkYEZ7fuNPqB","outputId":"56edc1fe-5997-45ec-839f-d8cce1306cad","execution":{"iopub.status.busy":"2021-07-16T15:29:39.323046Z","iopub.execute_input":"2021-07-16T15:29:39.323328Z","iopub.status.idle":"2021-07-16T15:29:39.375476Z","shell.execute_reply.started":"2021-07-16T15:29:39.3233Z","shell.execute_reply":"2021-07-16T15:29:39.374269Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize missing values\nmno.matrix(concrete_df, figsize = (20, 6));","metadata":{"id":"D8sahz2vNPqI","outputId":"bd49427e-feab-4f0a-c42e-5167cf0b548b","execution":{"iopub.status.busy":"2021-07-16T15:29:39.381533Z","iopub.execute_input":"2021-07-16T15:29:39.382077Z","iopub.status.idle":"2021-07-16T15:29:39.841439Z","shell.execute_reply.started":"2021-07-16T15:29:39.382026Z","shell.execute_reply":"2021-07-16T15:29:39.840024Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary statistics\nconcrete_df.describe().T","metadata":{"id":"P8VXLCUQNPqR","outputId":"76a589b2-ce84-41fc-f19f-f99e476f85da","execution":{"iopub.status.busy":"2021-07-16T15:29:39.845288Z","iopub.execute_input":"2021-07-16T15:29:39.845631Z","iopub.status.idle":"2021-07-16T15:29:39.888242Z","shell.execute_reply.started":"2021-07-16T15:29:39.845601Z","shell.execute_reply":"2021-07-16T15:29:39.886753Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '4.7'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.7 Data Cleaning Summary </strong></p> ","metadata":{}},{"cell_type":"markdown","source":"1. We had 25 duplicate instances in dataset and dropped those duplicates.\n2. We had outliers in 'Water', 'Superplastic', 'Fineagg', 'Age' and 'Strength' column also, handled these outliers by replacing every outlier with upper side of the whisker.\n3. We had outliers in 'Water' column also, handled these outliers by replacing every outlier with lower side of the whisker.\n4. No missing values in dataset.","metadata":{"id":"ApLGWHytNPqV"}},{"cell_type":"markdown","source":"<a id = '5.0'></a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 5. EDA (Data Analysis and Preparation) </h2> ","metadata":{}},{"cell_type":"markdown","source":"<a id = '5.1'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.1 Variable Identification </strong></p> ","metadata":{}},{"cell_type":"markdown","source":"* **Target variable:** 'Strength'\n* **Predictors (Input varibles):** 'Cement', 'Slag', 'Ash', 'Water', 'Superplastic', 'Coarseagg', 'Fineagg', 'Age'","metadata":{"id":"2rCQzIkgNPqV"}},{"cell_type":"markdown","source":"<a id = '5.2'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.2 Univariate Analysis </strong></p> ","metadata":{}},{"cell_type":"markdown","source":"#### Numerical columns - 'Cement', 'Slag', 'Ash', 'Water', 'Superplastic', 'Coarseagg', 'Fineagg', 'Age' and 'Strength'","metadata":{"id":"rNEIwU5TNPqV"}},{"cell_type":"code","source":"cols = [i for i in concrete_df.columns if i not in 'strength']\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\"]\nfig = plt.figure(figsize=(13,25))\n\nfor i,j,k in itertools.zip_longest(cols,range(length),cs):\n    plt.subplot(4,2,j+1)\n    ax = sns.distplot(concrete_df[i],color=k,rug=True)\n    ax.set_facecolor(\"w\")\n    plt.axvline(concrete_df[i].mean(),linestyle=\"dashed\",label=\"mean\",color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","metadata":{"id":"SZXELhNQNPqV","outputId":"dee86fba-e2ad-475a-86a7-b493d5bab325","execution":{"iopub.status.busy":"2021-07-16T15:29:39.890328Z","iopub.execute_input":"2021-07-16T15:29:39.890774Z","iopub.status.idle":"2021-07-16T15:29:43.089895Z","shell.execute_reply.started":"2021-07-16T15:29:39.890729Z","shell.execute_reply":"2021-07-16T15:29:43.088402Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for x in concrete_df:\n    #sns.distplot(concrete_df[x]);\n    #plt.title(\"{} distribution\".format(x))\n    #plt.show()    ","metadata":{"id":"FFFgYt4nNPql","execution":{"iopub.status.busy":"2021-07-16T15:29:43.091955Z","iopub.execute_input":"2021-07-16T15:29:43.092409Z","iopub.status.idle":"2021-07-16T15:29:43.097468Z","shell.execute_reply.started":"2021-07-16T15:29:43.092364Z","shell.execute_reply":"2021-07-16T15:29:43.095929Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Univariate analysis:**\n* **Cement column - Right skewed distribution -- cement is skewed to higher values**\n* **Slag column - Right skewed distribution -- slag is skewed to higher values and there are two gaussians**\n* **Ash column - Right skewed distribution -- ash is skewed to higher values and there are two gaussians**\n* **Water column - Moderately left skewed distribution**\n* **Superplastic column - Right skewed distribution -- superplastic is skewed to higher values and there are two gaussians**\n* **Coarseagg column - Moderately left skewed distribution**\n* **Fineagg column - Moderately left skewed distribution**\n* **Age column - Right skewed distribution -- age is skewed to higher values and there are five gaussians**","metadata":{"id":"AtmRkH_-NPql"}},{"cell_type":"markdown","source":"#### Concrete compressive strength distribution","metadata":{"id":"yMTKYI1dNPql"}},{"cell_type":"code","source":"plt.figure(figsize=(13,6))\nsns.distplot(concrete_df[\"strength\"],color=\"b\",rug=True)\nplt.axvline(concrete_df[\"strength\"].mean(), linestyle=\"dashed\",color=\"k\", label='mean',linewidth=2)\nplt.legend(loc=\"best\",prop={\"size\":14})\nplt.title(\"Concrete compressivee strength distribution\")\nplt.show()","metadata":{"id":"FnNAeDeDNPql","outputId":"a01e940b-926a-48a2-ce9b-126e88329840","execution":{"iopub.status.busy":"2021-07-16T15:29:43.099023Z","iopub.execute_input":"2021-07-16T15:29:43.099344Z","iopub.status.idle":"2021-07-16T15:29:43.434151Z","shell.execute_reply.started":"2021-07-16T15:29:43.099311Z","shell.execute_reply":"2021-07-16T15:29:43.433003Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Strength column seems to be uniformly distributed**","metadata":{"id":"Vn5kBEEXNPq1"}},{"cell_type":"markdown","source":"<a id = '5.3'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.3 Study Summary Statistics </strong></p> ","metadata":{}},{"cell_type":"code","source":"# Summary statistics\nconcrete_df.describe().T","metadata":{"id":"XUwqqAu9NPq1","outputId":"74cf8b3f-19df-4141-ab81-ae8c07316b77","execution":{"iopub.status.busy":"2021-07-16T15:29:43.435615Z","iopub.execute_input":"2021-07-16T15:29:43.435944Z","iopub.status.idle":"2021-07-16T15:29:43.473376Z","shell.execute_reply.started":"2021-07-16T15:29:43.435908Z","shell.execute_reply":"2021-07-16T15:29:43.472236Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **The above output prints the important summary statistics of all the numeric variables like the mean, median (50%), minimum, and maximum values, along with the standard deviation.**\n\n* **cement column - Right skewed distribution -- cement is skewed to higher values**\n* **slag column - Right skewed distribution -- slag is skewed to higher values and there are two gaussians**\n* **ash column - Right skewed distribution -- ash is skewed to higher values and there are two gaussians**\n* **water column - Moderately left skewed distribution**\n* **superplastic column - Right skewed distribution -- superplastic is skewed to higher values and there are two gaussians**\n* **coarseagg column - Moderately left skewed distribution**\n* **fineagg column - Moderately left skewed distribution**\n* **age column - Right skewed distribution -- age is skewed to higher values and there are five gaussians**\n* **strength column - Moderately right skewed distribution**","metadata":{"id":"q8IaXd84NPrE"}},{"cell_type":"markdown","source":"<a id = '5.4'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.4 Multivariate Analysis </strong></p> ","metadata":{}},{"cell_type":"code","source":"sns.pairplot(concrete_df, diag_kind = 'kde', corner = True);","metadata":{"id":"XkUSoKTWNPrI","outputId":"b66cda8a-bc19-4a01-8e3e-654e51f9a76f","execution":{"iopub.status.busy":"2021-07-16T15:29:43.474672Z","iopub.execute_input":"2021-07-16T15:29:43.474974Z","iopub.status.idle":"2021-07-16T15:29:54.350379Z","shell.execute_reply.started":"2021-07-16T15:29:43.474947Z","shell.execute_reply":"2021-07-16T15:29:54.346118Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Diagonals Analysis\n\n* If we look at KDE diagonal plots, there are at least 2 Gaussians (2 peaks) in Slag, Ash, Superplastic and Age, even though it's not unsupervised learning but in this dataset there are at least 2 clusters and there may be more.\n\n* Range of clusters in this dataset is 2 to 6.\n\n* The diagonal analysis give same insights as we got from univariate analysis.\n\n#### Off Diagonal Analysis: Relationship between indpendent attributes\n##### Scatter plots\n\n* Cement vs other independent attributes: This attribute does not have any significant relation with other independent features. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n\n* Slag vs other independent attributes: This attribute does not have any significant relation with other independent features. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n\n* Ash vs other independent attributes: This attribute does not have any significant relation with other independent features. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n\n* Water vs other independent attributes: This attribute have negative curvy-linear relationship with Fineagg, Coarseagg and Superplastic, as Water content increases means Fineagg, Coarseagg and Superplastic are reducing. It does not have any significant relationship with other independent atributes.\n\n* Superplastic vs other independent attributes:This attribute have negative linear relationship with water only. It does not have any significant relationship with other independent attributes.\n\n* Coarseagg vs other independent attributes: This attribute does not have any significant relation with other independent features. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n\n* Fineagg vs other independent attributes: It has negative linear relationship with water. It does not have any significant relation with any other attributes. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n\nThe reason why we are doing all this analysis is if we find any kind of dimensions which are very strongly correlated i.e. r value close to 1 or -1 such dimensions are giving same information to your algorithms, its a redundant dimension. So in such cases we may want to keep one and drop the other which we should keep and which we should drop depends on again your domain expertise, which one of the dimension is more prone to errors.I would like to drop that dimension. Or we have a choice to combine these dimensions and create a composite dimension out of it.\n\n\n#### Strength attribute : Relationship between dependent and independent attributes\n\n* Strength vs Cement: It is having curvy-linear relationship with concrete cement and it is good predictor of concrete strength.\n* Strength vs Slag: It is having very weak relationship with concrete slag because there are cloud of points(rectangular shape).\n* Strength vs Ash: It is having weak relationship with concrete ash because there are cloud of points(rectangular shape).\n* Strength vs Water: It is having curvy-linear relationship with water and it is good predictor of concrete strength.\n* Strength vs Superplastic: It is having weak relationship with superplastic because there are cloud of points(ballon shape) and it might be a good predictor of concrete strength..\n* Strength vs Coarseagg: It is having very weak relationship with concrete coarseagg because there are cloud of points(rectangular shape).\n* Strength vs Fineagg: It is having very weak relationship with concrete fineagg because there are cloud of points(rectangular shape).\n* Strength vs Age: It is having curvy-linear relationship with concrete age and it might be a good predictor of concrete strength.\n\n\n* **Finally Cement, Water, Superplastic and Age can be good predictors of concrete strength.**","metadata":{"id":"FtDHebscNPrQ"}},{"cell_type":"markdown","source":"<a id = '5.5'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.5 Study Correlation </strong></p> ","metadata":{}},{"cell_type":"code","source":"# Check the Correlation\nconcrete_df.corr()","metadata":{"id":"tAqdcvH6NPrT","outputId":"9e98bc11-5687-4e28-f3f0-7303f792ddc5","execution":{"iopub.status.busy":"2021-07-16T15:29:54.352587Z","iopub.execute_input":"2021-07-16T15:29:54.353033Z","iopub.status.idle":"2021-07-16T15:29:54.376039Z","shell.execute_reply.started":"2021-07-16T15:29:54.352987Z","shell.execute_reply":"2021-07-16T15:29:54.374832Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Pairplot for checking the Correlation","metadata":{"id":"usminojBNPrV"}},{"cell_type":"code","source":"sns.pairplot(concrete_df[['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age', 'strength']], kind = 'reg', corner = True);","metadata":{"id":"1RmQK2RmNPrV","outputId":"3c5621d4-f370-4f19-bbfa-eb6f35192a41","execution":{"iopub.status.busy":"2021-07-16T15:29:54.377969Z","iopub.execute_input":"2021-07-16T15:29:54.378365Z","iopub.status.idle":"2021-07-16T15:30:12.54637Z","shell.execute_reply.started":"2021-07-16T15:29:54.378253Z","shell.execute_reply":"2021-07-16T15:30:12.545126Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Heatmap for checking the Correlation","metadata":{"id":"sJi-CSnANPrV"}},{"cell_type":"code","source":"corr = abs(concrete_df.corr()) # correlation matrix\nlower_triangle = np.tril(corr, k = -1)  # select only the lower triangle of the correlation matrix\nmask = lower_triangle == 0  # to mask the upper triangle in the following heatmap\n\nplt.figure(figsize = (12,10))\nsns.heatmap(lower_triangle, center = 0.5, cmap = 'coolwarm', annot= True, xticklabels = corr.index, yticklabels = corr.columns,\n            cbar= True, linewidths= 1, mask = mask)   # Da Heatmap\nplt.show()","metadata":{"id":"mgBlLq7JNPrV","outputId":"847c9018-66c5-4f8c-9ee2-664a103a133f","execution":{"iopub.status.busy":"2021-07-16T15:30:12.548233Z","iopub.execute_input":"2021-07-16T15:30:12.548658Z","iopub.status.idle":"2021-07-16T15:30:13.002528Z","shell.execute_reply.started":"2021-07-16T15:30:12.548606Z","shell.execute_reply":"2021-07-16T15:30:13.001229Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n\n* **Looking at the Correlation table; 'Cement', 'Water', 'Superplastic' and 'Age' features are influencing the concrete strength.**\n\n\n* **Concrete strength feature is having Moderate Positive Correlation with Cement feature.**\n* **Concrete strength feature is having Low Positive Correlation with Superplastic and Age features**\n* **Concrete strength feature is having Low Positive Correlation with Water features**\n* **Concrete strength feature is having negligible Correlation with Slag, Ash, Coarseagg and Fineagg features**\n\n\n* **Water feature is having Moderate Positive Correlation with Superplastic feature**\n\n\n* **Concrete cement feature is having Low Positive Correlation with Slag and Ash features**\n\n\n* **Concrete fineagg feature is having Low Positive Correlation with Water feature**\n\n* **Concrete ash feature is having Low Positive Correlation with Superplastic feature**","metadata":{"id":"ecfB5YN6NPrl"}},{"cell_type":"markdown","source":"<a id = '5.6'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.6 EDA (Exploratory Data Analysis) Summary </strong></p> ","metadata":{}},{"cell_type":"markdown","source":"1. Except 'Cement', 'Water', 'Superplastic' and 'Age' features, all other features are having very weak relationship with concrete 'Strength' feature and does not account for making statistical decision (of correlation).\n\n2. Concrete Cement feature is having Low Positive Correlation with Slag and Ash features, perhaps we can create additional features like (cement + slag) and (cement + ash) to predict the concrete strength.\n\n3. Concrete Fineagg feature is having Low Positive Correlation with Water feature, perhaps we can create additional features like  (water + fineagg) to predict the concrete strength.\n\n4. Concrete Ash feature is having Low Positive Correlation with Superplastic feature, perhaps we can create additional features like  (ash + Superplastic) to predict the concrete strength.\n\n5. Range of clusters in this dataset is 2 to 6.","metadata":{"id":"4NRuVgnDNPrl"}},{"cell_type":"markdown","source":"<a id = '6.0'></a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 6. Feature Engineering </h2> ","metadata":{}},{"cell_type":"markdown","source":"<a id = '6.1'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 6.1 Variable Creation </strong></p> ","metadata":{}},{"cell_type":"markdown","source":"### Identify opportunities (if any) to create a composite feature, drop a feature etc.","metadata":{"id":"dy3qc8b7NPr2"}},{"cell_type":"markdown","source":"* As mentioned in EDA summary.\n\n**Independent features are influencing concrete strength - 'Cement', 'Water', 'Superplastic' and 'Age'**\n\n**Composite features are influencing concrete strength - cement + slag, cement + ash and water + fineagg. We can create these composite featues because these features are having some relationship within them.** \n\n**Note: Before concluding anything we can try with feature selection methods and then compare the resutls.**","metadata":{"id":"AeFVF-lWNPr4"}},{"cell_type":"markdown","source":"<a id = '7.0'></a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 7. Model Building and Validation </h2> ","metadata":{}},{"cell_type":"code","source":"import matplotlib.gridspec as gridspec\n\n# sns styling figures\nsns.set(style='white')\nsns.set(style='whitegrid',color_codes=True)","metadata":{"id":"tZmkUE5fNPr-","execution":{"iopub.status.busy":"2021-07-16T15:30:13.004111Z","iopub.execute_input":"2021-07-16T15:30:13.00449Z","iopub.status.idle":"2021-07-16T15:30:13.012356Z","shell.execute_reply.started":"2021-07-16T15:30:13.004452Z","shell.execute_reply":"2021-07-16T15:30:13.010625Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '7.1'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.1 Sampling Techniques - Create Training and Test Set </strong></p> ","metadata":{}},{"cell_type":"code","source":"X = concrete_df.drop(['strength'], axis = 1) # Considering all Predictors\ny = concrete_df['strength']","metadata":{"id":"J095W9bmNPsQ","execution":{"iopub.status.busy":"2021-07-16T15:30:13.014247Z","iopub.execute_input":"2021-07-16T15:30:13.014791Z","iopub.status.idle":"2021-07-16T15:30:13.029572Z","shell.execute_reply.started":"2021-07-16T15:30:13.014741Z","shell.execute_reply":"2021-07-16T15:30:13.028225Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 1)","metadata":{"id":"cdlwbW1ZNPsW","execution":{"iopub.status.busy":"2021-07-16T15:30:13.031405Z","iopub.execute_input":"2021-07-16T15:30:13.031833Z","iopub.status.idle":"2021-07-16T15:30:13.051392Z","shell.execute_reply.started":"2021-07-16T15:30:13.031793Z","shell.execute_reply":"2021-07-16T15:30:13.050467Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('X_train shape : ({0},{1})'.format(X_train.shape[0], X_train.shape[1]))\nprint('y_train shape : ({0},)'.format(y_train.shape[0]))\nprint('X_test shape : ({0},{1})'.format(X_test.shape[0], X_test.shape[1]))\nprint('y_test shape : ({0},)'.format(y_test.shape[0]))","metadata":{"id":"CzbFENhFNPsc","outputId":"1ccf8036-aca3-4827-f42f-cb6fdce5d1e6","execution":{"iopub.status.busy":"2021-07-16T15:30:13.052572Z","iopub.execute_input":"2021-07-16T15:30:13.052941Z","iopub.status.idle":"2021-07-16T15:30:13.06878Z","shell.execute_reply.started":"2021-07-16T15:30:13.052906Z","shell.execute_reply":"2021-07-16T15:30:13.066688Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '7.2'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.2 Decide on complexity of the model, should it be simple linear model in terms of parameters or would a quadratic or higher degree help </strong></p> ","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Train and test model </strong></p> ","metadata":{}},{"cell_type":"code","source":"def train_test_model(model, method, X_train, X_test, y_train, y_test, of_type, index, scale):\n    \n    print (model)\n    print (\"***************************************************************************\")\n    \n    if scale == 'yes':\n        # prepare the model with input scaling\n        pipeline = Pipeline([('scaler', PowerTransformer()), ('model', model)])\n    elif scale == 'no':\n        # prepare the model with input scaling\n        pipeline = Pipeline([('model', model)])\n\n    pipeline.fit(X_train, y_train) # Fit the model on Training set\n    prediction = pipeline.predict(X_test) # Predict on Test set\n\n    r2 = metrics.r2_score(y_test, prediction) # Calculate the r squared value on the Test set\n    rmse = np.sqrt(metrics.mean_squared_error(y_test, prediction)) # Root mean squared error\n    \n    if of_type == \"coef\":\n        # Intercept and Coefficients\n        print(\"The intercept for our model is {}\".format(model.intercept_), \"\\n\")\n        \n        for idx, col_name in enumerate(X_train.columns):\n            print(\"The coefficient for {} is {}\".format(col_name, model.coef_.ravel()[idx]))\n    \n    # Accuracy of Training data set\n    train_accuracy_score = pipeline.score(X_train, y_train)\n    \n    # Accuracy of Test data set\n    test_accuracy_score = pipeline.score(X_test, y_test)\n    \n    print (\"***************************************************************************\")\n    \n    if of_type == \"coef\":\n        \n        # FEATURE IMPORTANCES plot\n        plt.figure(figsize=(13,12))\n        plt.subplot(211)\n        print(model.coef_)\n        coef = pd.DataFrame(np.sort(model.coef_)[::-1].ravel())\n        coef[\"feat\"] = X_train.columns\n        ax1 = sns.barplot(coef[\"feat\"],coef[0],palette=\"jet_r\", linewidth=2)\n        ax1.set_facecolor(\"lightgrey\")\n        ax1.axhline(0,color=\"k\",linewidth=2)\n        plt.ylabel(\"coefficients\")\n        plt.xlabel(\"features\")\n        plt.title(method + ' ' + 'FEATURE IMPORTANCES')\n    \n    elif of_type == \"feat\":\n        \n        # FEATURE IMPORTANCES plot\n        plt.figure(figsize=(13,12))\n        plt.subplot(211)\n        coef = pd.DataFrame(np.sort(model.feature_importances_)[::-1])\n        coef[\"feat\"] = X_train.columns\n        ax2 = sns.barplot(coef[\"feat\"], coef[0],palette=\"jet_r\", linewidth=2)\n        ax2.set_facecolor(\"lightgrey\")\n        ax2.axhline(0,color=\"k\",linewidth=2)\n        plt.ylabel(\"coefficients\")\n        plt.xlabel(\"features\")\n        plt.title(method + ' ' + 'FEATURE IMPORTANCES')\n    \n    # Store the accuracy results for each model in a dataframe for final comparison\n    resultsDf = pd.DataFrame({'Method': method, 'R Squared': r2, 'RMSE': rmse, 'Train Accuracy': train_accuracy_score, \n                              'Test Accuracy': test_accuracy_score}, index=[index])\n    \n    return resultsDf  # return all the metrics along with predictions","metadata":{"id":"20KSqm4eNPsm","execution":{"iopub.status.busy":"2021-07-16T15:30:13.070566Z","iopub.execute_input":"2021-07-16T15:30:13.071132Z","iopub.status.idle":"2021-07-16T15:30:13.092604Z","shell.execute_reply.started":"2021-07-16T15:30:13.071081Z","shell.execute_reply":"2021-07-16T15:30:13.091012Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Train and test all models </strong></p> ","metadata":{}},{"cell_type":"code","source":"def train_test_allmodels(X_train_common, X_test_common, y_train, y_test, scale):\n    # define regressor models\n    models=[['LinearRegression',LinearRegression()],\n        ['Ridge',Ridge(random_state = 1)],\n        ['Lasso',Lasso(random_state = 1)],\n        ['KNeighborsRegressor',KNeighborsRegressor(n_neighbors = 3)],\n        ['SVR',SVR(kernel = 'linear')],\n        ['RandomForestRegressor',RandomForestRegressor(random_state = 1)],\n        ['BaggingRegressor',BaggingRegressor(random_state = 1)],\n        ['ExtraTreesRegressor',ExtraTreesRegressor(random_state = 1)],\n        ['AdaBoostRegressor',AdaBoostRegressor(random_state = 1)],\n        ['GradientBoostingRegressor',GradientBoostingRegressor(random_state = 1)],\n        ['CatBoostRegressor',CatBoostRegressor(random_state = 1, verbose=False)],\n        ['XGBRegressor',XGBRegressor()]\n    ]\n\n    resultsDf_common = pd.DataFrame()\n    i = 1\n    for name, regressor in models:\n        # Train and Test the model\n        reg_resultsDf = train_test_model(regressor, name, X_train_common, X_test_common, y_train, y_test, 'none', i, scale)\n\n        # Store the accuracy results for each model in a dataframe for final comparison\n        resultsDf_common = pd.concat([resultsDf_common, reg_resultsDf])\n        i = i+1\n\n    return resultsDf_common","metadata":{"id":"wcZuGPmroO6t","execution":{"iopub.status.busy":"2021-07-16T15:30:13.094571Z","iopub.execute_input":"2021-07-16T15:30:13.094969Z","iopub.status.idle":"2021-07-16T15:30:13.11443Z","shell.execute_reply.started":"2021-07-16T15:30:13.094937Z","shell.execute_reply":"2021-07-16T15:30:13.112498Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Model with Hyperparameter Tuning </strong></p> ","metadata":{}},{"cell_type":"code","source":"def hyperparameterstune_model(name, model, X_train, y_train, param_grid):\n    \n    start = time.time()  # note the start time \n    \n    # define grid search\n    cv = KFold(n_splits=10, random_state=1)\n    #grid_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, n_jobs=-1, cv=cv, \n                                     #scoring = 'neg_root_mean_squared_error', error_score=0)\n    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, \n                                     scoring = 'neg_root_mean_squared_error', error_score=0)\n    model_grid_result = grid_search.fit(X_train, y_train)\n\n    # summarize results\n    print(name, \"- Least: RMSE %f using %s\" % (model_grid_result.best_score_ * (-1), model_grid_result.best_params_))\n    \n    end = time.time()  # note the end time\n    duration = end - start  # calculate the total duration\n    print(\"Total duration\" , duration, \"\\n\")\n    \n    return model_grid_result.best_estimator_","metadata":{"id":"nzai8WD8NPsu","execution":{"iopub.status.busy":"2021-07-16T15:30:13.116626Z","iopub.execute_input":"2021-07-16T15:30:13.117051Z","iopub.status.idle":"2021-07-16T15:30:13.135305Z","shell.execute_reply.started":"2021-07-16T15:30:13.117015Z","shell.execute_reply":"2021-07-16T15:30:13.133896Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Modelling - Linear Regression </strong></p> ","metadata":{}},{"cell_type":"code","source":"# Building a Linear Regression model\nlr = LinearRegression()\n                                                     \n# Train and Test the model\nresultsDf = train_test_model(lr, 'LinearRegression', X_train, X_test, y_train, y_test, 'none', 1, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf","metadata":{"id":"S2L4pIspNPs_","outputId":"2b7fc43d-d859-4483-9093-e7f1338ecc96","execution":{"iopub.status.busy":"2021-07-16T15:30:13.137128Z","iopub.execute_input":"2021-07-16T15:30:13.137456Z","iopub.status.idle":"2021-07-16T15:30:13.22425Z","shell.execute_reply.started":"2021-07-16T15:30:13.137425Z","shell.execute_reply":"2021-07-16T15:30:13.223209Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation: This model performs better on training set and poorly on test set which shows that it's an overfitting and very complex model.**\n\n","metadata":{"id":"ONA0M-nY9AiR"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Linear Regression using Statsmodels </strong></p> ","metadata":{"id":"s1xmKysPNPtE"}},{"cell_type":"code","source":"# R^2 is not a reliable metric as it always increases with addition of more attributes even if the attributes have no \n# influence on the predicted variable. Instead we use adjusted R^2 which removes the statistical chance that improves R^2\n\n# OLS library expects the X and y to be given in one single dataframe\nconcrete_df_train = pd.concat([X_train, y_train], axis=1)\nconcrete_df_train.head()\n\nlr_ols = smf.ols(formula= 'strength ~ cement + slag + ash + water + superplastic + coarseagg + fineagg + age', \n              data = concrete_df_train).fit()\n\nprint(lr_ols.summary())  # Inferential statistics","metadata":{"id":"IlHN3D9KNPtF","outputId":"ed6a9130-a0c1-4657-fe60-0d7dc8f191cc","execution":{"iopub.status.busy":"2021-07-16T15:30:13.225893Z","iopub.execute_input":"2021-07-16T15:30:13.226207Z","iopub.status.idle":"2021-07-16T15:30:13.269705Z","shell.execute_reply.started":"2021-07-16T15:30:13.226176Z","shell.execute_reply":"2021-07-16T15:30:13.268464Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Statistical Outputs:**\n\n* R-squared and Adj. R-squared are very close, it is sign that all predictors are relevant to the overall model.\n\n* F-statistic = 240.8 is large value of F-statistic and p-value = 1.64e-194 is very close to 0 and also it is less than 0.05 hence we can reject null hypothesis. That means there is evidence that there is good amount of linear relationship between target variable (Strength) and all predictors.\n\n**Parameters Estimates and the Associated Statistical Tests:**\n\n* By looking into OLS summary coefficients column results, they are same as sklearn linear model coefficients and even intercept is same.\n\n* By looking into OLS summary t-test columns results: so for constant variable ie -0.564, we have a p-value = 0.573 which is greater than 0.05 then we accept the null hypothesis.\n\n* By looking into OLS summary t-test columns results: Cement, Slag, Ash, Water, Coarseagg and Fineagg  and Superplastic are having p-value < 0.05 becuase we are testing t-test at 95% confidence interval, so we reject null hypothesis and accepth alternate hypothesis. That means that there is evidence that these predictors are having good amount of linear relationship with target variable.\n\n* By looking into OLS summary t-test columns results: Coarseagg and Fineagg  and Superplastic are having p-value > 0.05 becuase we are testing t-test at 95% confidence interval, so we accept null hypothesis. That means that there is evidence these predictors are not having good amount of linear relationship with target variable.\n\n* By looking into OLS summary t-test columns results: std err reflects the level of accuracy of the coefficients. std err values are very close to 0 except intercept that means the level of accuracy is high.\n\n**Residul Tests Results:**\n\n* Skew: 0.076, there is small tail to left in the residuals distribution.\n* Kurtosis: 3.285, there is a peak in the residuals distribution.\n* Prob(Omnibus): 0.225, Prob(JB): 0.216 - indicates that p-value > 0.05 meaning it's not siginificant and data is normally distributed.\n* The condition number is large, 1.05e+05. This indicates that some of the features are collinear.","metadata":{"id":"ErGqS6kgNPtK"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Ridge Regression </strong></p> ","metadata":{"id":"C78OxkIZ7Ubw"}},{"cell_type":"code","source":"# Building a Ridge Regression model\nrr = Ridge(random_state = 1)\n\n# Train and Test the model\nrr_resultsDf = train_test_model(rr, 'Ridge', X_train, X_test, y_train, y_test, 'coef', 2, 'yes')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,rr_resultsDf])\nresultsDf","metadata":{"id":"RpfPAbvN7W_n","outputId":"e651e0d3-9155-435f-bb0e-12fb76167488","execution":{"iopub.status.busy":"2021-07-16T15:30:13.271414Z","iopub.execute_input":"2021-07-16T15:30:13.272061Z","iopub.status.idle":"2021-07-16T15:30:13.677716Z","shell.execute_reply.started":"2021-07-16T15:30:13.272013Z","shell.execute_reply":"2021-07-16T15:30:13.676478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation: This model performs better on training set as well as test set and RMSE is als reduced to 6.77**\n\n","metadata":{"id":"dl-2g5Db-3a9"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Lasso Regression </strong></p>","metadata":{"id":"Z5MElLBR7clv"}},{"cell_type":"code","source":"# Building a Lasso Regression model\nlasso = Lasso(random_state = 1)\n\n# Train and Test the model\nlasso_resultsDf = train_test_model(lasso, 'Lasso', X_train, X_test, y_train, y_test, 'coef', 3, 'yes')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf, lasso_resultsDf])\nresultsDf","metadata":{"id":"b1F-bXxf7fgQ","outputId":"60ffd752-e8ed-4599-fc17-d4a090049a47","execution":{"iopub.status.busy":"2021-07-16T15:30:13.67935Z","iopub.execute_input":"2021-07-16T15:30:13.679663Z","iopub.status.idle":"2021-07-16T15:30:14.062786Z","shell.execute_reply.started":"2021-07-16T15:30:13.679635Z","shell.execute_reply":"2021-07-16T15:30:14.061637Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation: This model performs better on training set and performance drops on test set which shows that it's an overfitting and very complex model.**","metadata":{"id":"v7pcA36J_WFR"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Adding Interaction Terms - Linear Regression </strong></p>","metadata":{"id":"aALsULdeNPtL"}},{"cell_type":"code","source":"# Transfom X_train and X_test to polynomial features\npipe = Pipeline([('scaler', PowerTransformer()), ('polynomial', PolynomialFeatures(degree = 2, interaction_only=True))])\nX_train_poly2 = pd.DataFrame(pipe.fit_transform(X_train))\nX_test_poly2 = pd.DataFrame(pipe.fit_transform(X_test))","metadata":{"id":"05LsMQTtNPtM","execution":{"iopub.status.busy":"2021-07-16T15:30:14.064481Z","iopub.execute_input":"2021-07-16T15:30:14.065143Z","iopub.status.idle":"2021-07-16T15:30:14.136062Z","shell.execute_reply.started":"2021-07-16T15:30:14.065095Z","shell.execute_reply":"2021-07-16T15:30:14.135113Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and Test the model\nlr_resultsDf = train_test_model(lr, 'Linear Regression with interaction features', X_train_poly2, X_test_poly2, y_train, y_test, \n                                'none', 4, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,lr_resultsDf])\nresultsDf","metadata":{"id":"MldZzla0NPte","outputId":"26129c7e-1b19-451b-b49b-ccd1533beb1d","execution":{"iopub.status.busy":"2021-07-16T15:30:14.13753Z","iopub.execute_input":"2021-07-16T15:30:14.138156Z","iopub.status.idle":"2021-07-16T15:30:14.200339Z","shell.execute_reply.started":"2021-07-16T15:30:14.138105Z","shell.execute_reply":"2021-07-16T15:30:14.199209Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After adding interaction terms we have 66 columns which includes original 8 columns and others which are created from original columns.**\n\n**Notice that by adding interaction terms, RMSE decreased from 8.65 to 6.20**\n\n**When we have more columns are less rows then we are likely to be in overfit zone because model coefficients is having high values. Hence let's try with non-regularized models.**","metadata":{"id":"Cuzq0Jh-NPtj"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Fit a simple non regularized linear model on interaction terms - Ridge Regression </strong></p>","metadata":{}},{"cell_type":"code","source":"# Building a Ridge Regression model\nrr = Ridge(random_state = 1)\n\n# Train and Test the model\nrr_resultsDf = train_test_model(rr, 'Ridge with interaction features', X_train_poly2, X_test_poly2, y_train, y_test, 'coef', 5, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,rr_resultsDf])\nresultsDf","metadata":{"id":"4VieyNLKNPtl","outputId":"eb4e1b44-5b52-47ae-af85-075b635a2322","execution":{"iopub.status.busy":"2021-07-16T15:30:14.202369Z","iopub.execute_input":"2021-07-16T15:30:14.203177Z","iopub.status.idle":"2021-07-16T15:30:15.068657Z","shell.execute_reply.started":"2021-07-16T15:30:14.203121Z","shell.execute_reply":"2021-07-16T15:30:15.067709Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation: Notice that test accuracy is better than Linear regression with interaction features. This model performs better on training set and performance drops on test set which shows that it's an overfitting and very complex model.**","metadata":{"id":"qxO-BG3lAer4"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Fit a simple non regularized linear model on interaction terms - Lasso Regression </strong></p>","metadata":{}},{"cell_type":"code","source":"# Building a Lasso Regression model\nlasso = Lasso(random_state = 1)\n\n# Train and Test the model\nlasso_resultsDf = train_test_model(lasso, 'Lasso with interaction features', X_train_poly2, X_test_poly2, y_train, y_test, 'coef', 6, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf, lasso_resultsDf])\nresultsDf","metadata":{"id":"tUCVv59JNPuo","outputId":"57c472a0-3bea-4b42-e479-f2b5db7b4665","execution":{"iopub.status.busy":"2021-07-16T15:30:15.070091Z","iopub.execute_input":"2021-07-16T15:30:15.070593Z","iopub.status.idle":"2021-07-16T15:30:16.420424Z","shell.execute_reply.started":"2021-07-16T15:30:15.070558Z","shell.execute_reply":"2021-07-16T15:30:16.418731Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation: This model performs better on training set and poorly on test set which shows that it's an overfitting and very complex model.**","metadata":{"id":"csTuK-rIA3Qo"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Polynomial Linear Regression </strong></p>","metadata":{"id":"CziD66mHNPu6"}},{"cell_type":"markdown","source":"**Let's try polynomial model on the same data from 1 to 5 degree polynomial features**","metadata":{"id":"6dsMRH1UNPu7"}},{"cell_type":"code","source":"for i in range(1,6):\n    pipe = Pipeline([('scaler', PowerTransformer()), ('polynomial', PolynomialFeatures(degree = i)), \n                 ('model', LinearRegression())])\n    pipe.fit(X_train, y_train) # Fit the model on Training set\n    prediction = pipe.predict(X_test) # Predict on Test set\n\n    r2 = metrics.r2_score(y_test, prediction) # Calculate the r squared value on the Test set\n    rmse = np.sqrt(metrics.mean_squared_error(y_test, prediction)) # Root mean squared error\n    \n    print (\"R-Squared for {0} degree polynomial is {1}\".format(i, r2))\n    print (\"ROOT MEAN SQUARED ERROR for {0} degree polynomial features is {1}\".format(i, rmse),\"\\n\")","metadata":{"id":"aqW751RzNPu8","outputId":"74623430-5581-4dee-eda6-52f96fd44d3a","execution":{"iopub.status.busy":"2021-07-16T15:30:16.422311Z","iopub.execute_input":"2021-07-16T15:30:16.42282Z","iopub.status.idle":"2021-07-16T15:30:17.098763Z","shell.execute_reply.started":"2021-07-16T15:30:16.422768Z","shell.execute_reply":"2021-07-16T15:30:17.096628Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**By looking at the above results, RMSE is start increasing from 1 degree polynomial which has 6.77 RMSE and RMSE came down to 5.50 for 2 degree polynomial features. Again from 3 degree polynomial RMSE is starts increasing hence optimal degree of polynomial is 2 degree polynomial.**\n\n**Let's try 2-degree polynomial model on the same data**","metadata":{"id":"RyfsR6DqNPvC"}},{"cell_type":"code","source":"pipe = Pipeline([('scaler', PowerTransformer()), ('polynomial', PolynomialFeatures(degree = 2)), \n                 ('model', LinearRegression())])\n    \npipe.fit(X_train, y_train) # Fit the model on Training set\nprediction = pipe.predict(X_test) # Predict on Test set\n        \nr2 = metrics.r2_score(y_test, prediction) # Calculate the r squared value on the Test set\nrmse = np.sqrt(metrics.mean_squared_error(y_test, prediction)) # Root mean squared error\n\nprint (\"R-Squared :\", r2)\nprint (\"ROOT MEAN SQUARED ERROR :\", rmse)\n\n# Accuracy of Training data set\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(pipe.score(X_train, y_train)))\n\n# Accuracy of Test data set\naccuracy_score = pipe.score(X_test, y_test)\nprint(\"Accuracy of Test data set: {0:.4f} %\".format(accuracy_score))","metadata":{"id":"xW-sJoEYNPvD","outputId":"9e3da210-420f-4dbb-baf6-72222053a827","execution":{"iopub.status.busy":"2021-07-16T15:30:17.10087Z","iopub.execute_input":"2021-07-16T15:30:17.10129Z","iopub.status.idle":"2021-07-16T15:30:17.238278Z","shell.execute_reply.started":"2021-07-16T15:30:17.101243Z","shell.execute_reply":"2021-07-16T15:30:17.235596Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the accuracy results for each model in a dataframe for final comparison\npoly_resultsDf = pd.DataFrame({'Method': 'Linear Regression with Polynomial features', 'R Squared': r2, 'RMSE': rmse, 'Train Accuracy': pipe.score(X_train, y_train), \n                          'Test Accuracy': accuracy_score}, index=[7])\nresultsDf = pd.concat([resultsDf, poly_resultsDf])\nresultsDf","metadata":{"id":"QyBXpM3FKepr","outputId":"0af21943-51e8-417a-fd9f-aecd3b8acc18","execution":{"iopub.status.busy":"2021-07-16T15:30:17.240289Z","iopub.execute_input":"2021-07-16T15:30:17.240792Z","iopub.status.idle":"2021-07-16T15:30:17.273932Z","shell.execute_reply.started":"2021-07-16T15:30:17.240743Z","shell.execute_reply":"2021-07-16T15:30:17.272788Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**By looking at above results, 2-degree polynomial model is performs better on training set and even on test set with 1% difference which shows that it's like to be sweet spot. Henc let's try with non-regularized models.**","metadata":{"id":"iRAWDKFzNPvO"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Fit a simple non regularized linear model on polynomial features - Ridge Regression </strong></p>","metadata":{}},{"cell_type":"code","source":"# Transfom X_train and X_test to polynomial features\npipe = Pipeline([('scaler', PowerTransformer()), ('polynomial', PolynomialFeatures(degree = 2))])\nX_train_poly_2 = pd.DataFrame(pipe.fit_transform(X_train))\nX_test_poly_2 = pd.DataFrame(pipe.fit_transform(X_test))","metadata":{"id":"2KL6DWxwNPvQ","execution":{"iopub.status.busy":"2021-07-16T15:30:17.275673Z","iopub.execute_input":"2021-07-16T15:30:17.276337Z","iopub.status.idle":"2021-07-16T15:30:17.385835Z","shell.execute_reply.started":"2021-07-16T15:30:17.276287Z","shell.execute_reply":"2021-07-16T15:30:17.384823Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building a Ridge Regression model\nrr = Ridge(random_state = 1)\n\n# Train and Test the model\nrr_resultsDf = train_test_model(rr, 'Ridge with polynomial features', X_train_poly_2, X_test_poly_2, y_train, y_test, 'coef', 8, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,rr_resultsDf])\nresultsDf","metadata":{"id":"lS6kJTbbNPvV","outputId":"301c4cf0-f5fe-4b0a-d1de-36685e0cb2d6","execution":{"iopub.status.busy":"2021-07-16T15:30:17.387437Z","iopub.execute_input":"2021-07-16T15:30:17.387746Z","iopub.status.idle":"2021-07-16T15:30:18.490312Z","shell.execute_reply.started":"2021-07-16T15:30:17.387715Z","shell.execute_reply":"2021-07-16T15:30:18.488779Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation: This model performs better on training set and poorly on test set which shows that it's an overfitting and very complex model.**","metadata":{"id":"UZqyAHmCC2_m"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Fit a simple non regularized linear model on polynomial features - Lasso Regression </strong></p>","metadata":{"id":"Y01-mOybNPvZ"}},{"cell_type":"code","source":"# Building a Lasso Regression model\nlasso = Lasso(random_state = 1)\n\n# Train and Test the model\nlasso_resultsDf = train_test_model(lasso, 'Lasso with polynomial features', X_train_poly_2, X_test_poly_2, y_train, y_test, 'coef', 9, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf, lasso_resultsDf])\nresultsDf","metadata":{"id":"y7_MlIi8NPva","outputId":"376bf1bb-7a6b-4dc9-a1b0-3de814d7bd56","execution":{"iopub.status.busy":"2021-07-16T15:30:18.491679Z","iopub.execute_input":"2021-07-16T15:30:18.49203Z","iopub.status.idle":"2021-07-16T15:30:19.56928Z","shell.execute_reply.started":"2021-07-16T15:30:18.491998Z","shell.execute_reply":"2021-07-16T15:30:19.568485Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation: This model performs better on training set and poorly on test set which shows that it's an overfitting and very complex model.**","metadata":{"id":"zeO9V3VNC_wl"}},{"cell_type":"markdown","source":"<a id = '7.3'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.3 Explore for gaussians. If data is likely to be a mix of gaussians, explore individual clusters and presenting my findings in terms of the independent attributes and their suitability to predict strength </strong></p> ","metadata":{"id":"mhBS0o40NPvf"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> K Means Clustering </strong></p>","metadata":{"id":"65LHadl1NPvg"}},{"cell_type":"code","source":"# Scale the data using PowerTransformer\nscale = PowerTransformer()\nconcrete_df_scaled = pd.DataFrame(scale.fit_transform(concrete_df))","metadata":{"id":"8Gj7WzRFNPvh","execution":{"iopub.status.busy":"2021-07-16T15:30:19.571041Z","iopub.execute_input":"2021-07-16T15:30:19.571352Z","iopub.status.idle":"2021-07-16T15:30:19.617665Z","shell.execute_reply.started":"2021-07-16T15:30:19.571324Z","shell.execute_reply":"2021-07-16T15:30:19.616263Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_range = range(1, 15)  \ncluster_errors = []\nfor num_clusters in cluster_range:\n    clusters = KMeans(n_clusters = num_clusters, n_init = 5, random_state = 1)\n    clusters.fit(concrete_df_scaled)\n    \n    labels = clusters.labels_\n    centroids = clusters.cluster_centers_\n    \n    cluster_errors.append(clusters.inertia_ )\n\nclusters_df = pd.DataFrame({ \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","metadata":{"id":"g3M6pRAtNPvl","outputId":"05228cf7-b537-4aa2-bf50-ca3dd2f736e1","execution":{"iopub.status.busy":"2021-07-16T15:30:19.619239Z","iopub.execute_input":"2021-07-16T15:30:19.619552Z","iopub.status.idle":"2021-07-16T15:30:35.230648Z","shell.execute_reply.started":"2021-07-16T15:30:19.619512Z","shell.execute_reply":"2021-07-16T15:30:35.229783Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Elbow plot\nplt.figure(figsize=(12,6))\nplt.plot(clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" );","metadata":{"id":"7o-utDTfNPvr","outputId":"9a3f38b7-19d6-49ec-ba16-0237a78dcec4","execution":{"iopub.status.busy":"2021-07-16T15:30:35.232384Z","iopub.execute_input":"2021-07-16T15:30:35.232958Z","iopub.status.idle":"2021-07-16T15:30:35.541804Z","shell.execute_reply.started":"2021-07-16T15:30:35.232917Z","shell.execute_reply":"2021-07-16T15:30:35.540884Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# k = 6\ncluster = KMeans(n_clusters = 6, random_state = 1)\ncluster.fit(concrete_df_scaled)","metadata":{"id":"sgoDhDkhNPvw","outputId":"2a6c9126-6f92-4312-f711-a990b1476e57","execution":{"iopub.status.busy":"2021-07-16T15:30:35.543621Z","iopub.execute_input":"2021-07-16T15:30:35.544274Z","iopub.status.idle":"2021-07-16T15:30:35.67852Z","shell.execute_reply.started":"2021-07-16T15:30:35.54423Z","shell.execute_reply":"2021-07-16T15:30:35.677005Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a new column \"GROUP\" which will hold the cluster id of each record\nprediction=cluster.predict(concrete_df_scaled)\nconcrete_df_scaled[\"GROUP\"] = prediction","metadata":{"id":"PqSASKUfNPv1","execution":{"iopub.status.busy":"2021-07-16T15:30:35.683151Z","iopub.execute_input":"2021-07-16T15:30:35.683544Z","iopub.status.idle":"2021-07-16T15:30:35.700835Z","shell.execute_reply.started":"2021-07-16T15:30:35.683508Z","shell.execute_reply":"2021-07-16T15:30:35.699423Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"centroids = cluster.cluster_centers_\ncentroids","metadata":{"id":"l8-0hYWuNPv_","outputId":"a3a6fb85-4d03-4c56-9e8e-de90c01093f4","execution":{"iopub.status.busy":"2021-07-16T15:30:35.704754Z","iopub.execute_input":"2021-07-16T15:30:35.705132Z","iopub.status.idle":"2021-07-16T15:30:35.719082Z","shell.execute_reply.started":"2021-07-16T15:30:35.705099Z","shell.execute_reply":"2021-07-16T15:30:35.717931Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"centroid_df = pd.DataFrame(centroids, columns = list(concrete_df))\ncentroid_df","metadata":{"id":"KIpKGa_uNPwF","outputId":"1e997f38-790f-4b8b-a447-144a31ddee2d","execution":{"iopub.status.busy":"2021-07-16T15:30:35.723695Z","iopub.execute_input":"2021-07-16T15:30:35.724063Z","iopub.status.idle":"2021-07-16T15:30:35.747469Z","shell.execute_reply.started":"2021-07-16T15:30:35.724031Z","shell.execute_reply":"2021-07-16T15:30:35.746431Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Instead of interpreting the neumerical values of the centroids, let us do a visual analysis by converting the \n## centroids and the data in the cluster into box plots.\nconcrete_df_scaled.boxplot(by = 'GROUP',  layout=(3,3), figsize=(15, 10));","metadata":{"id":"4Re2RANyNPwP","outputId":"ce970b37-6985-48bb-a5fc-9cc60e429b15","execution":{"iopub.status.busy":"2021-07-16T15:30:35.751354Z","iopub.execute_input":"2021-07-16T15:30:35.751685Z","iopub.status.idle":"2021-07-16T15:30:40.719212Z","shell.execute_reply.started":"2021-07-16T15:30:35.751654Z","shell.execute_reply":"2021-07-16T15:30:40.717865Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Here, None of the dimensions are good predictor of target variable.\n* For all the dimensions (variables) every cluster have a similar range of values except in one case.\n* We can see that the body of the cluster are overlapping.\n* So in k means, though, there are clusters in datasets on different dimensions. But we can not see any distinct characteristics of these clusters which tell us to break data into different clusters and build separate models for them.","metadata":{"id":"ADBtULmINPwV"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> KNN Regressor </strong></p>","metadata":{"id":"nwCR8NYVNPwX"}},{"cell_type":"markdown","source":"#### Transform original data","metadata":{"id":"zVqvvoZPNPwY"}},{"cell_type":"code","source":"def train_test_transform(X_train, X_test):\n    scale = PowerTransformer()\n    \n    X_train_scaled = pd.DataFrame(scale.fit_transform(X_train))\n    X_test_scaled = pd.DataFrame(scale.fit_transform(X_test))\n    \n    return X_train_scaled, X_test_scaled","metadata":{"id":"xkn6GOg3NPwZ","execution":{"iopub.status.busy":"2021-07-16T15:30:40.720725Z","iopub.execute_input":"2021-07-16T15:30:40.721078Z","iopub.status.idle":"2021-07-16T15:30:40.72737Z","shell.execute_reply.started":"2021-07-16T15:30:40.721044Z","shell.execute_reply":"2021-07-16T15:30:40.725868Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# empty list that will hold error\nerror = []\n\nX_train_scaled, X_test_scaled = train_test_transform(X_train, X_test)\n\n# perform error metrics for values from 1,2,3....29\nfor k in range(1,30):\n    \n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train_scaled, y_train)\n    # predict the response\n    y_pred = knn.predict(X_test_scaled)\n    error.append(np.mean(y_pred != y_test))","metadata":{"id":"AcX9-umKNPwo","execution":{"iopub.status.busy":"2021-07-16T15:30:40.728966Z","iopub.execute_input":"2021-07-16T15:30:40.729376Z","iopub.status.idle":"2021-07-16T15:30:41.06013Z","shell.execute_reply.started":"2021-07-16T15:30:40.729341Z","shell.execute_reply":"2021-07-16T15:30:41.058865Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.plot(range(1,30), error, color='red', linestyle='dashed',marker='o',markerfacecolor='blue',markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean error')","metadata":{"id":"JzBXVjdSNPw2","outputId":"c27b6673-1470-4f6d-e70e-e26a6f47ede2","execution":{"iopub.status.busy":"2021-07-16T15:30:41.061792Z","iopub.execute_input":"2021-07-16T15:30:41.062234Z","iopub.status.idle":"2021-07-16T15:30:41.337053Z","shell.execute_reply.started":"2021-07-16T15:30:41.06219Z","shell.execute_reply":"2021-07-16T15:30:41.335475Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Optimal value of K is 2","metadata":{"id":"Tbe4mbTgERDR"}},{"cell_type":"code","source":"# Building a KNN Regression model\nknn = KNeighborsRegressor(n_neighbors = 2)\n\n# Train and Test the model\nknn_resultsDf = train_test_model(knn, 'KNeighborsRegressor', X_train, X_test, y_train, y_test, 'none', 10, 'yes')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf, knn_resultsDf])\nresultsDf","metadata":{"id":"YbP2uhP3NPw7","outputId":"1840f26d-82c9-426c-9c2a-6d3277d0c447","execution":{"iopub.status.busy":"2021-07-16T15:30:41.338675Z","iopub.execute_input":"2021-07-16T15:30:41.339023Z","iopub.status.idle":"2021-07-16T15:30:41.417549Z","shell.execute_reply.started":"2021-07-16T15:30:41.338991Z","shell.execute_reply":"2021-07-16T15:30:41.416296Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation: This model performs better on training set and poorly on test set which shows that it's an overfitting and very complex model.**","metadata":{"id":"MqVIKFPPEhhw"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Build SupportVectorRegressor, RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, CatBoostRegressor and XGBRegressor models </strong></p>","metadata":{"id":"pF_WXe_11LyK"}},{"cell_type":"code","source":"# define regressor models\nmodels=[\n    ['SVR',SVR(kernel='linear')],\n    ['DecisionTreeRegressor', DecisionTreeRegressor(random_state = 1)],\n    ['RandomForestRegressor',RandomForestRegressor(random_state = 1)],\n    ['BaggingRegressor',BaggingRegressor(random_state = 1)],\n    ['ExtraTreesRegressor',ExtraTreesRegressor(random_state = 1)],\n    ['AdaBoostRegressor',AdaBoostRegressor(random_state = 1)],\n    ['GradientBoostingRegressor',GradientBoostingRegressor(random_state = 1)],\n    ['CatBoostRegressor',CatBoostRegressor(random_state = 1, verbose=False)],\n    ['XGBRegressor',XGBRegressor()]\n]\n\n\ni = 11\nfor name, regressor in models:\n    if name == 'SVR':\n        # Train and Test the model\n        svr_resultsDf = train_test_model(regressor, name, X_train, X_test, y_train, y_test, 'coef', i, 'yes')\n\n        # Store the accuracy results for each model in a dataframe for final comparison\n        resultsDf = pd.concat([resultsDf, svr_resultsDf])\n    elif name == 'BaggingRegressor':\n        # Train and Test the model\n        bag_resultsDf = train_test_model(regressor, name, X_train, X_test, y_train, y_test, 'none', i, 'yes')\n\n        # Store the accuracy results for each model in a dataframe for final comparison\n        resultsDf = pd.concat([resultsDf, bag_resultsDf])\n    else:\n        # Train and Test the model\n        ensemble_resultsDf = train_test_model(regressor, name, X_train, X_test, y_train, y_test, 'feat', i, 'yes')\n\n        # Store the accuracy results for each model in a dataframe for final comparison\n        resultsDf = pd.concat([resultsDf, ensemble_resultsDf])\n    i = i+1","metadata":{"id":"LmogTNSE1WNP","outputId":"29184754-4622-498d-9529-e4c9e41454ba","execution":{"iopub.status.busy":"2021-07-16T15:30:41.419105Z","iopub.execute_input":"2021-07-16T15:30:41.419414Z","iopub.status.idle":"2021-07-16T15:30:47.344319Z","shell.execute_reply.started":"2021-07-16T15:30:41.419375Z","shell.execute_reply":"2021-07-16T15:30:47.343082Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### By looking at above feature importnce from ensemble models:\n**Cement, Slag, Ash, Water, Superplastic, Coarsegg and fineagg are top important features**","metadata":{"id":"3dP-AjTKEynb"}},{"cell_type":"code","source":"# Show results dataframe\nresultsDf","metadata":{"id":"hZ2Xbbbi_l7C","outputId":"286d6691-8286-403f-ac36-859b582f1ab3","execution":{"iopub.status.busy":"2021-07-16T15:30:47.345745Z","iopub.execute_input":"2021-07-16T15:30:47.34607Z","iopub.status.idle":"2021-07-16T15:30:47.362051Z","shell.execute_reply.started":"2021-07-16T15:30:47.346039Z","shell.execute_reply":"2021-07-16T15:30:47.360661Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '7.4'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.4 Overall Summary - Before feature selection </strong></p>\n\nI am able to predict the Concrete compressive strength using few ingradients with below details.\n\nRefer the above table:\n\n1. I have tried with simple linear regression which is overfit model hence I moved on to non-regularized models.\n\n    a. Ridge performs better on both training and test set.\n\n    b. Lasso performs better on training set and poorly on test set.\n2. As mentioned in Multi-variate analysis, there is some interaction between independent features hence I have tried with simple linear regression and non-regularized models (Ridge and Lasso) and all of them turned out to be overfit models.\n\n3. As mentioned in Multi-variate analysis, there are some non-linear(curvy-linear) relatioship within independent features as well as with target variable hence I have tried with polynomial features.\n\n    a. Simple linear regression with polynomial features with degree = 2 performs better on both training and test set with 1% difference.\n\n    b. Ridge and Lasso with polynomial features turned out to be overfit models.\n\n4. I have tried with Support Vector Regressor and it performs better on both training and test set.\n\n5. I have tried with KNeighborsRegressor, DecisionTreeRegressor, RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, CatBoostRegressor and XGBRegressor models, sad news is all these models turned out to be overfit models.\n\n6. Best models are as follows:\n\n    a. Linear Regression with Polynomial features - Test accuracy = 86.94% with RMSE = 5.50\n\n    b. Ridge regression with original features - Test accuracy = 80.24% with RMSE = 6.77\n\n    c. SVR with original features - Test accuracy = 80.03% with RMSE = 6.81","metadata":{"id":"qkyWffytFdkh"}},{"cell_type":"markdown","source":"<a id = '8.0'></a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 8. Feature Selection Methods </h2> ","metadata":{}},{"cell_type":"markdown","source":"<a id = '8.1'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 8.1 Feature Importance </strong></p> ","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Lasso with it's important features </strong></p> ","metadata":{"id":"-A51ypZGhggH"}},{"cell_type":"code","source":"# Selecting features using Lasso regularisation using SelectFromModel\nsel_ = SelectFromModel(Lasso(random_state = 1))\nsel_.fit(X_train, y_train)\n\n# Visualising features that were kept by the lasso regularisation\nsel_.get_support()\n\n# Make a list of with the selected features\nlasso_selected_feat = X_train.columns[(sel_.get_support())]\n\n# Prepare train and test data\nX_train_lasso = X_train[lasso_selected_feat]\nX_test_lasso = X_test[lasso_selected_feat]\n\n# Lasso with it's important features\nX_train_lasso.columns","metadata":{"id":"VXrQXBpYefdY","outputId":"cc6ca478-4781-4ef8-d0df-3060a6d39611","execution":{"iopub.status.busy":"2021-07-16T15:30:47.369895Z","iopub.execute_input":"2021-07-16T15:30:47.370263Z","iopub.status.idle":"2021-07-16T15:30:47.39106Z","shell.execute_reply.started":"2021-07-16T15:30:47.370205Z","shell.execute_reply":"2021-07-16T15:30:47.389738Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> All models - Lasso with it's important features </strong></p> ","metadata":{"id":"rgcRXUMLiwyX"}},{"cell_type":"code","source":"# Train and Test all models with Lasso interaction terms\ntrain_test_allmodels(X_train_lasso, X_test_lasso, y_train, y_test, 'no')","metadata":{"id":"HXMfFkiona6N","outputId":"19de49c5-da58-463b-ef37-eff21b523529","execution":{"iopub.status.busy":"2021-07-16T15:30:47.393498Z","iopub.execute_input":"2021-07-16T15:30:47.393944Z","iopub.status.idle":"2021-07-16T15:31:03.832912Z","shell.execute_reply.started":"2021-07-16T15:30:47.393901Z","shell.execute_reply":"2021-07-16T15:31:03.831647Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lasso with it's important features - observation: All these models performs better on training set and poorly on test set which shows that it's an overfitting and very complex models.**","metadata":{"id":"52C2x6r8Sade"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Lasso with it's polynomial important features </strong></p>","metadata":{"id":"MR2WdhXEovY7"}},{"cell_type":"code","source":"# Selecting features using Lasso regularisation using SelectFromModel\nsel_ = SelectFromModel(Lasso(random_state = 1))\nsel_.fit(X_train_poly_2, y_train)\n\n# Visualising features that were kept by the lasso regularisation\nsel_.get_support()\n\n# Make a list of with the selected features\nlasso_poly_selected_feat = X_train_poly_2.columns[(sel_.get_support())]\n\n# Prepare train and test data\nX_train_lasso_poly = X_train_poly_2[lasso_poly_selected_feat]\nX_test_lasso_poly = X_test_poly_2[lasso_poly_selected_feat]\n\n# Lasso with it's polynomial important features\nX_train_lasso_poly.columns","metadata":{"id":"HaoR2XAuoz2Y","outputId":"eac3cbec-42bc-4e9a-f291-d76676d1e5ad","execution":{"iopub.status.busy":"2021-07-16T15:31:03.834317Z","iopub.execute_input":"2021-07-16T15:31:03.83461Z","iopub.status.idle":"2021-07-16T15:31:03.849622Z","shell.execute_reply.started":"2021-07-16T15:31:03.834582Z","shell.execute_reply":"2021-07-16T15:31:03.848681Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> All models - Lasso with it's polynomial important features </strong></p>","metadata":{"id":"0jl6iWa6pbV-"}},{"cell_type":"code","source":"# Train and Test all models with Lasso interaction terms\ntrain_test_allmodels(X_train_lasso_poly, X_test_lasso_poly, y_train, y_test, 'no')","metadata":{"id":"FtZRp4e4pVVb","outputId":"4bba0dfa-3447-490b-99a8-32e9f0053e50","execution":{"iopub.status.busy":"2021-07-16T15:31:03.85086Z","iopub.execute_input":"2021-07-16T15:31:03.851202Z","iopub.status.idle":"2021-07-16T15:31:06.57281Z","shell.execute_reply.started":"2021-07-16T15:31:03.851172Z","shell.execute_reply":"2021-07-16T15:31:06.57107Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lasso with it's polynomial important features - observation: All these models performs better on training set and poorly on test set which shows that it's an overfitting and very complex models.**","metadata":{"id":"ciTaI07SUrjP"}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Importance features from ensemble models - DecisionTree, ExtraTree, AdaBoost, GradientBoost, CatBoost and XGBoost </strong></p>\n\n**As mentioned in earlier steps, top 5 important features from ensemble models are - cement, slag, ash, water and superplastic**","metadata":{"id":"_Ci8jGZny_Vt"}},{"cell_type":"code","source":"# Select top 5 important features\nX_feat = concrete_df[['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg', 'age']]\ny = concrete_df['strength']\n\n# Split data into train and test set\nX_train_feat, X_test_feat, y_train, y_test = train_test_split(X_feat, y, test_size = 0.30, random_state = 1)","metadata":{"id":"4vcrtofLzMSe","execution":{"iopub.status.busy":"2021-07-16T15:31:06.574665Z","iopub.execute_input":"2021-07-16T15:31:06.575004Z","iopub.status.idle":"2021-07-16T15:31:06.585677Z","shell.execute_reply.started":"2021-07-16T15:31:06.574975Z","shell.execute_reply":"2021-07-16T15:31:06.584199Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> All models with ensemble's important features </strong></p>","metadata":{"id":"Ucn1JXng013H"}},{"cell_type":"code","source":"# Train and Test all models with Lasso interaction terms\ntrain_test_allmodels(X_train_feat, X_test_feat, y_train, y_test, 'yes')","metadata":{"id":"YHkFeISR0tnZ","outputId":"829ab07e-5c52-4716-92cd-b9135890d77b","execution":{"iopub.status.busy":"2021-07-16T15:31:06.587601Z","iopub.execute_input":"2021-07-16T15:31:06.588228Z","iopub.status.idle":"2021-07-16T15:31:09.902606Z","shell.execute_reply.started":"2021-07-16T15:31:06.588171Z","shell.execute_reply":"2021-07-16T15:31:09.90067Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**All models with ensemble's important features - observation:**\n\n1. Linear regression - Test accuracy = 79.50% with RMSE = 6.90\n2. Ridge regression - Test accuracy = 79.49% with RMSE = 6.90\n3. SVR - Test accuracy = 79.34% with RMSE = 6.92\n\nRest all models performs better on training set and poorly on test set which shows that it's an overfitting and very complex models.","metadata":{"id":"AbwbwB5wVZZX"}},{"cell_type":"markdown","source":"<a id = '8.2'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 8.2 Overall summary - after feature selection </strong></p>\n\nI am able to predict the concrete compressive strength using few ingradients with below details.\n\nI have tried three different feature importance methods and please find the below results.\n\n1. Lasso with it's important features are as follows:\n ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg', 'fineagg', 'age']\n\n I have built all models with above features and all of them performs better on training set and poorly on test set which shows that it's an overfitting and very complex models.\n\n2. Lasso with it's polynomial important features are as follows:\n [1, 2, 4, 5, 7, 8, 26, 44]\n\n I have built all models with above features and all of them performs better on training set and poorly on test set which shows that it's an overfitting and very complex models.\n\n3. All models with ensemble's important features\n ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg', 'age']\n\n I have built all models with above features and all of them performs better on training set and poorly on test set except below models which shows that it's an overfitting and very complex models.\n\n    a. Linear regression - Test accuracy = 79.50% with RMSE = 6.90\n\n    b. Ridge regression - Test accuracy = 79.49% with RMSE = 6.90\n\n    c. SVR - Test accuracy = 79.34% with RMSE = 6.92","metadata":{"id":"geCF5syjWHTl"}},{"cell_type":"markdown","source":"<a id = '8.3'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 8.3 Comparison of with and without feature selection methods </strong></p>\n\n1. Best models without feature selection methods:\n\n    a. Linear Regression with Polynomial features - Test accuracy = 86.94% with RMSE = 5.50\n\n    b. Ridge regression with original features - Test accuracy = 80.24% with RMSE = 6.77\n\n    c. SVR with original features - Test accuracy = 80.03% with RMSE = 6.81\n\n\n2. Best models with ensemble's feature selection methods:\n\n    a. Linear regression - Test accuracy = 79.50% with RMSE = 6.90\n\n    b. Ridge regression - Test accuracy = 79.49% with RMSE = 6.90\n\n    c. SVR - Test accuracy = 79.34% with RMSE = 6.92\n\n**By comparing both options, I see the best models which suits this project are from without feature selection methods, I mean with original features.**","metadata":{"id":"0SlGdALUZqKW"}},{"cell_type":"markdown","source":"<a id = '9.0'></a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 9. Optimization </h2> ","metadata":{}},{"cell_type":"markdown","source":"**As mentioned in above comparisons, we can hypertune the parameters for Ridge and Support Vector Regressor algorithms.**","metadata":{"id":"sxTLVfcex2W2"}},{"cell_type":"markdown","source":"<a id = '9.1'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 9.1 Hyper Parameter Tuning </strong></p>","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Ridge and SVR models - Hyperparameter tuning with original features </strong></p>","metadata":{"id":"05EYWBc5ALDi"}},{"cell_type":"code","source":"# define regressor models\nmodels=[['Ridge',Ridge()],\n    #['Lasso',Lasso()],\n    #['KNeighborsRegressor',KNeighborsRegressor()],\n    ['SVR',SVR()]\n    #['RandomForestRegressor',RandomForestRegressor()],\n    #['BaggingRegressor',BaggingRegressor()],\n    #['ExtraTreesRegressor',ExtraTreesRegressor()],\n    #['AdaBoostRegressor',AdaBoostRegressor()],\n    #['GradientBoostingRegressor',GradientBoostingRegressor()],\n    #['CatBoostRegressor',CatBoostRegressor(verbose=False)],\n    #['XGBRegressor',XGBRegressor()]\n]\n\n# define model parameters\nridge_param_grid = {'alpha': [1,0.1,0.01,0.001,0.0001,0]}\nlasso_param_grid = {'alpha': [0.02, 0.024, 0.025, 0.026, 0.03]}\nknn_param_grid = {'n_neighbors': range(3, 21, 2),\n                 'weights': ['uniform', 'distance'],\n                 'metric': ['euclidean', 'manhattan', 'minkowski']}\nsvr_param_grid = {'kernel': ['poly', 'rbf', 'sigmoid'],\n                 'C': [50, 10, 1.0, 0.1, 0.01],\n                 'gamma': ['scale']}\nrf_param_grid = {'n_estimators': [10, 100, 1000],\n                 'max_features': ['auto', 'sqrt', 'log2']}\nbag_param_grid = {'n_estimators': [10, 100, 1000],\n                 'max_samples': np.arange(0.7, 0.8, 0.05)}\net_param_grid = {'n_estimators': np.arange(10,100,10),\n                 'max_features': ['auto', 'sqrt', 'log2'],\n                 'min_samples_split': np.arange(2,15,1)}\nadb_param_grid = {'n_estimators': np.arange(30,100,10),\n                 'learning_rate': np.arange(0.1,1,0.5)}\ngb_param_grid = {'n_estimators': np.arange(30,100,10),\n                 'learning_rate': np.arange(0.1,1,0.5)}\ncatb_param_grid = {'depth': [4, 7, 10],\n                  'learning_rate' : [0.03, 0.1, 0.15],\n                  'l2_leaf_reg': [1,4,9],\n                  'iterations': [300]}\nxgb_param_grid = {'learning_rate': [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n                  'max_depth' : [ 3, 4, 5, 6, 8, 10, 12, 15],\n                  'min_child_weight': [ 1, 3, 5, 7],\n                  'gamma': [0.0, 0.1, 0.2 , 0.3, 0.4],\n                  'colsample_bytree': [ 0.3, 0.4, 0.5 , 0.7]}\n\n\nfor name, regressor in models:\n    if name == 'Ridge':\n        ridge_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, ridge_param_grid)\n    elif name == 'Lasso':\n        lasso_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, lasso_param_grid)\n    elif name == 'KNeighborsRegressor':\n        knn_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, knn_param_grid)\n    elif name == 'SVR':\n        svr_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, svr_param_grid)\n    elif name == 'RandomForestRegressor':\n        rf_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, rf_param_grid)\n    elif name == 'BaggingRegressor':\n        bag_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, bag_param_grid)\n    elif name == 'ExtraTreesRegressor':\n        et_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, et_param_grid)\n    elif name == 'AdaBoostRegressor':\n        adb_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, adb_param_grid)\n    elif name == 'GradientBoostingRegressor':\n        gb_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, gb_param_grid)\n    elif name == 'CatBoostRegressor':\n        catb_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, catb_param_grid)\n    elif name == 'XGBRegressor':\n        xgb_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, xgb_param_grid)","metadata":{"id":"YOTg2qppNPyL","outputId":"8f141294-2a88-48d0-be3c-1cf4f72e82e9","execution":{"iopub.status.busy":"2021-07-16T15:31:09.904306Z","iopub.execute_input":"2021-07-16T15:31:09.904647Z","iopub.status.idle":"2021-07-16T15:31:15.098875Z","shell.execute_reply.started":"2021-07-16T15:31:09.904608Z","shell.execute_reply":"2021-07-16T15:31:15.097763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> Ridge and SVR models with Hyperparameters </strong></p>","metadata":{"id":"OdsJ_WSbkUlJ"}},{"cell_type":"code","source":"# define regressor models\nmodels=[['Ridge', ridge_best_estimator],\n    #['Lasso', lasso_best_estimator],\n    #['KNeighborsRegressor', knn_best_estimator],\n    ['SVR', svr_best_estimator],\n    #['RandomForestRegressor', rf_best_estimator],\n    #['BaggingRegressor', bag_best_estimator],\n    #['ExtraTreesRegressor',et_best_estimator],\n    #['AdaBoostRegressor', adb_best_estimator],\n    #['GradientBoostingRegressor', gb_best_estimator],\n    #['CatBoostRegressor', catb_best_estimator],\n    #['XGBRegressor', xgb_best_estimator]\n]\n\nresultsDf_hp = pd.DataFrame()\ni = 1\nfor name, regressor in models:\n    # Train and Test the model\n    resultsDf_hp_ind = train_test_model(regressor, name, X_train, X_test, y_train, y_test, 'none', i, 'yes')\n\n    # Store the accuracy results for each model in a dataframe for final comparison\n    resultsDf_hp = pd.concat([resultsDf_hp, resultsDf_hp_ind])\n    i = i+1\n\n# Show results dataframe\nresultsDf_hp","metadata":{"id":"UKisNCMgkVft","outputId":"7c47e78d-b107-4d0c-92a2-502361da5a48","execution":{"iopub.status.busy":"2021-07-16T15:31:15.100459Z","iopub.execute_input":"2021-07-16T15:31:15.101056Z","iopub.status.idle":"2021-07-16T15:31:15.463912Z","shell.execute_reply.started":"2021-07-16T15:31:15.101009Z","shell.execute_reply":"2021-07-16T15:31:15.462652Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '9.2'></a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 9.2 Bootstrap Sampling - Model performance range at 95% confidence level</strong></p>","metadata":{}},{"cell_type":"code","source":"# Drop K-means cluster group from concrete_df_scaled dataset\nconcrete_df_scaled.drop(columns=['GROUP'], axis=1, inplace=True)","metadata":{"id":"6A9TPQE69KbF","execution":{"iopub.status.busy":"2021-07-16T15:31:15.465362Z","iopub.execute_input":"2021-07-16T15:31:15.465696Z","iopub.status.idle":"2021-07-16T15:31:15.47338Z","shell.execute_reply.started":"2021-07-16T15:31:15.465664Z","shell.execute_reply":"2021-07-16T15:31:15.471915Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> GradientBoostingRegressor</strong></p>","metadata":{"id":"VrgN1GA_IwKD"}},{"cell_type":"code","source":"values = concrete_df_scaled.values\n\n# Number of bootstrap samples to create\nn_iterations = 1000        \n\n# size of a bootstrap sample\nn_size = int(len(concrete_df_scaled) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\ngbm_stats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n\n     # fit model\n    gbmTree = GradientBoostingRegressor(n_estimators=50)\n\n    # fit against independent variables and corresponding target values\n    gbmTree.fit(train[:,:-1], train[:,-1]) \n\n    # Take the target column for all rows in test set\n    y_bs_test = test[:,-1]  \n\n    # evaluate model\n    # predict based on independent variables in the test data\n    score = gbmTree.score(test[:, :-1] , y_bs_test)\n    predictions = gbmTree.predict(test[:, :-1])  \n\n    gbm_stats.append(score)","metadata":{"id":"8aCfOoBS7mRM","execution":{"iopub.status.busy":"2021-07-16T15:31:15.475061Z","iopub.execute_input":"2021-07-16T15:31:15.47543Z","iopub.status.idle":"2021-07-16T15:43:22.440662Z","shell.execute_reply.started":"2021-07-16T15:31:15.475384Z","shell.execute_reply":"2021-07-16T15:43:22.439286Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot scores\nplt.hist(gbm_stats)\nplt.show()\n\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(gbm_stats, p))  \np = (alpha+((1.0-alpha)/2.0)) * 100\nupper = min(1.0, np.percentile(gbm_stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","metadata":{"id":"Y6a6uQ-i_2gY","outputId":"e8884224-ee82-4e9d-93c4-bfd85813239a","execution":{"iopub.status.busy":"2021-07-16T15:43:22.443219Z","iopub.execute_input":"2021-07-16T15:43:22.443683Z","iopub.status.idle":"2021-07-16T15:43:22.691726Z","shell.execute_reply.started":"2021-07-16T15:43:22.443633Z","shell.execute_reply":"2021-07-16T15:43:22.690399Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color: #007580 \"><strong> RandomForestRegressor </strong></p>","metadata":{"id":"7xdzXEBiIzPF"}},{"cell_type":"code","source":"values = concrete_df_scaled.values\n\n# Number of bootstrap samples to create\nn_iterations = 1000        \n\n# size of a bootstrap sample\nn_size = int(len(concrete_df_scaled) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nrf_stats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n     # fit model\n    rfTree = RandomForestRegressor(n_estimators=100)\n\n    # fit against independent variables and corresponding target values\n    rfTree.fit(train[:,:-1], train[:,-1]) \n\n    # Take the target column for all rows in test set\n    y_bs_test = test[:,-1]  \n\n    # evaluate model\n    # predict based on independent variables in the test data\n    score = rfTree.score(test[:, :-1] , y_bs_test)\n    predictions = rfTree.predict(test[:, :-1]) \n\n    rf_stats.append(score)","metadata":{"id":"pe8DexxyC7ud","execution":{"iopub.status.busy":"2021-07-16T15:43:22.693404Z","iopub.execute_input":"2021-07-16T15:43:22.693707Z","iopub.status.idle":"2021-07-16T16:02:31.843859Z","shell.execute_reply.started":"2021-07-16T15:43:22.693678Z","shell.execute_reply":"2021-07-16T16:02:31.842214Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot scores\nplt.hist(rf_stats)\nplt.show()\n\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(rf_stats, p))  \np = (alpha+((1.0-alpha)/2.0)) * 100\nupper = min(1.0, np.percentile(rf_stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","metadata":{"id":"uxqCejHfGzE2","outputId":"4104ddc0-d084-422c-ee4a-4c83def0fd62","execution":{"iopub.status.busy":"2021-07-16T16:02:31.845637Z","iopub.execute_input":"2021-07-16T16:02:31.846053Z","iopub.status.idle":"2021-07-16T16:02:32.100757Z","shell.execute_reply.started":"2021-07-16T16:02:31.846002Z","shell.execute_reply":"2021-07-16T16:02:32.09938Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '10.0'></a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 10. Conclusion </h2> ","metadata":{}},{"cell_type":"markdown","source":"1. I am able to predict the concrete compressive strength using original features with an accuracy of 86.94% on test data with RMSE = 5.50\n\n2. If we look at the above results from various methods then we got the best accuracy from original features and followed below steps to gain that much of accuracy.\n\n    a. As mentioned in Multi-variate analysis, there are some non-linear(curvy-linear) relatioship within independent features as well as with target variable hence I have tried with polynomial features.\n\n    b. Simple linear regression with polynomial features with degree = 2 performs better on both training and test set with 1% difference.\n\n3. We had 25 duplicate instances in dataset and dropped those duplicates.\n\n4. We had outliers in 'Water', 'Superplastic', 'Fineagg', 'Age' and 'Strength' column also, handled these outliers by replacing every outlier with upper and lower side of the whisker.\n\n5. Except 'Cement', 'Water', 'Superplastic' and 'Age' features, all other features are having very weak relationship with concrete 'Strength' feature and does not account for making statistical decision (of correlation).\n\n6. Range of clusters in this dataset is 2 to 6\n\n7. No missing values in dataset.\n\n8. Standardization of data using PowerTransformer improves accuracy slightly.\n\n9. Bootstrap sampling with GradientBoostingRegressor model performance is between 84.8% - 89.0% is better than other Regression algorithms.\n\n10. Bootstrap sampling with RandomForestRegressor model performance is between 86.8% - 91.6% is better than other Regression algorithms.\n\n11. **Finally Bootstrap sampling with RandomForestRegressor model with an accuracy of 86.6% - 91.6% is our best model.**\n\n<p style = \"font-size:30px; color: #007580 \"><strong> Thanks for reading.</strong></p>","metadata":{"id":"BoMIv4BvzBPe"}}]}